#!/usr/bin/env python3
"""
Enhanced JSON URL Processor with Individual URL Summarization
å„URLã‚’å€‹åˆ¥ã«è¦ç´„ã—ã¦ã‹ã‚‰çµ±åˆã™ã‚‹JSONãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
"""

import json
import time
import requests
from pathlib import Path
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, urlparse
import re
from bs4 import BeautifulSoup
from loguru import logger

class IndividualJSONUrlProcessor:
    """
    å„URLã‚’å€‹åˆ¥ã«è¦ç´„ã—ã¦ã‹ã‚‰çµ±åˆã™ã‚‹JSONãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
    """
    
    def __init__(self):
        self.session = requests.Session()
        # æœ€æ–°ã®User-Agentã¨è¿½åŠ ãƒ˜ãƒƒãƒ€ãƒ¼ã§403ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.max_content_length = 5000  # å„URLã®æœ€å¤§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã‚’å‰Šæ¸›
        self.request_timeout = 20  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—å»¶é•·
        self.batch_size = 3  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å‰Šæ¸›ã—ã¦å®‰å®šæ€§å‘ä¸Š
        self.max_retries = 2  # ãƒªãƒˆãƒ©ã‚¤å›æ•°è¿½åŠ 
        
        # LLM summarizeråˆæœŸåŒ–ã‚’è©¦è¡Œ
        self.llm_summarizer = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """LLMè¦ç´„æ©Ÿèƒ½ã‚’åˆæœŸåŒ–"""
        try:
            from src.summarizer_enhanced import LLMSummarizer
            from config.settings import Settings
            from config.llama2_config import LLAMA2_MODEL_PATH
            
            settings = Settings()
            model_path = Path(LLAMA2_MODEL_PATH)
            
            if model_path.exists():
                logger.info("ğŸ¤– Initializing LLM for individual URL summarization...")
                self.llm_summarizer = LLMSummarizer(
                    model_path=str(model_path),
                    settings=settings
                )
                logger.success("âœ… LLM summarizer initialized for individual processing")
            else:
                logger.warning("âš ï¸ LLM model not found - using basic summarization")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize LLM: {e}")
    
    def process_json_file_individually(self, json_file_path: Path) -> Dict[str, Any]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å„URLã‚’å€‹åˆ¥ã«å‡¦ç†ã—ã¦è¦ç´„ï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰"""
        try:
            logger.info(f"ğŸ“„ Processing JSON file with optimized individual URL summarization: {json_file_path}")
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(json_file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # URLã‚’æŠ½å‡º
            urls = self._extract_urls_from_json(json_data)
            logger.info(f"ğŸ”— Found {len(urls)} URLs to process individually (batch size: {self.batch_size})")
            
            # ãƒãƒƒãƒå‡¦ç†ã§å„URLã‚’å€‹åˆ¥ã«å‡¦ç†
            individual_summaries = []
            successful_count = 0
            total_batches = (len(urls) + self.batch_size - 1) // self.batch_size
            
            for batch_num in range(total_batches):
                start_idx = batch_num * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(urls))
                batch_urls = urls[start_idx:end_idx]
                
                logger.info(f"ğŸ”„ Processing batch {batch_num + 1}/{total_batches} ({len(batch_urls)} URLs)")
                
                # ãƒãƒƒãƒå†…ã®å„URLã‚’å‡¦ç†
                for i, url_info in enumerate(batch_urls):
                    url_idx = start_idx + i + 1
                    logger.info(f"ğŸŒ Processing URL {url_idx}/{len(urls)}: {url_info['url']}")
                    
                    try:
                        # URLã®å†…å®¹ã‚’å–å¾—
                        content = self._fetch_url_content(url_info['url'])
                        if content:
                            # å€‹åˆ¥ã«è¦ç´„
                            summary = self._summarize_individual_url(url_info, content)
                            individual_summaries.append(summary)
                            successful_count += 1
                            logger.success(f"âœ… Completed processing: {summary['name']}")
                        else:
                            logger.warning(f"âš ï¸ Failed to fetch: {url_info['url']}")
                            
                        # ãƒ¡ãƒ¢ãƒªè§£æ”¾ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                        self._cleanup_memory()
                        time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’çŸ­ç¸®
                        
                    except Exception as e:
                        logger.error(f"âŒ Error processing URL {url_info['url']}: {e}")
                        continue
                
                # ãƒãƒƒãƒå‡¦ç†å¾Œã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                logger.info(f"ğŸ§¹ Cleaning up memory after batch {batch_num + 1}")
                self._cleanup_memory()
                
                # ãƒãƒƒãƒé–“ã®çŸ­ã„ä¼‘æ†©
                if batch_num < total_batches - 1:
                    time.sleep(1)
            
            # å€‹åˆ¥è¦ç´„ã‚’çµ±åˆ
            logger.info(f"ğŸ“ Integrating {len(individual_summaries)} individual summaries...")
            integrated_summary = self._integrate_individual_summaries(
                individual_summaries, 
                json_file_path,
                len(urls),
                successful_count
            )
            
            return {
                'source_file': str(json_file_path),
                'total_urls': len(urls),
                'successful_summaries': successful_count,
                'individual_summaries': individual_summaries,
                'integrated_summary': integrated_summary,
                'processing_type': 'optimized_individual_url_summarization',
                'batch_size': self.batch_size
            }
            
        except Exception as e:
            logger.error(f"âŒ Error processing JSON file individually: {e}")
            raise
    
    def _summarize_individual_url(self, url_info: Dict[str, str], content: str) -> Dict[str, Any]:
        """å€‹åˆ¥URLã®å†…å®¹ã‚’è¦ç´„"""
        try:
            # LLMã«ã‚ˆã‚‹è¦ç´„
            if self.llm_summarizer and len(content) > 100:
                try:
                    logger.info(f"ğŸ¤– Creating LLM summary for: {url_info.get('name', 'Document')}")
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã™ãã‚‹å ´åˆã¯é©åº¦ã«åˆ‡ã‚Šè©°ã‚
                    summary_content = content[:1500] if len(content) > 1500 else content
                    
                    summary = self.llm_summarizer.summarize_english_to_english(
                        summary_content,
                        summary_type="brief"  # å‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚briefä½¿ç”¨
                    )
                    
                    if summary and len(summary.strip()) > 10:
                        logger.success(f"âœ… LLM summary generated for {url_info.get('name', 'Document')}")
                        return {
                            'name': url_info.get('name', 'Document'),
                            'url': url_info['url'],
                            'source': url_info.get('source', 'unknown'),
                            'category': url_info.get('category', 'Document'),
                            'summary': summary,
                            'content_length': len(content),
                            'summary_type': 'llm_generated'
                        }
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM summarization failed for {url_info['url']}: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªè¦ç´„
            basic_summary = self._create_basic_summary(content)
            
            return {
                'name': url_info.get('name', 'Document'),
                'url': url_info['url'],
                'source': url_info.get('source', 'unknown'),
                'category': url_info.get('category', 'Document'),
                'summary': basic_summary,
                'content_length': len(content),
                'summary_type': 'basic_extraction'
            }
            
        except Exception as e:
            logger.error(f"âŒ Error summarizing URL {url_info['url']}: {e}")
            return {
                'name': url_info.get('name', 'Document'),
                'url': url_info['url'],
                'source': url_info.get('source', 'unknown'),
                'category': url_info.get('category', 'Document'),
                'summary': f"Error processing content: {str(e)}",
                'content_length': 0,
                'summary_type': 'error'
            }
    
    def _create_basic_summary(self, content: str) -> str:
        """åŸºæœ¬çš„ãªè¦ç´„ã‚’ä½œæˆ"""
        if not content or len(content) < 50:
            return "Content too short for summarization."
        
        # æ–‡ç« ã‚’åˆ†å‰²
        sentences = [s.strip() for s in re.split(r'[.!?]+', content) if s.strip()]
        
        # æœ€åˆã®3æ–‡ã‚’è¦ç´„ã¨ã—ã¦ä½¿ç”¨
        summary_sentences = sentences[:3] if len(sentences) >= 3 else sentences
        summary = '. '.join(summary_sentences)
        
        if summary and not summary.endswith('.'):
            summary += '.'
        
        # æœ€å¤§400æ–‡å­—ã«åˆ¶é™
        if len(summary) > 200:
            summary = summary[:200] + "..."
        
        return summary if summary else "Unable to generate basic summary."
    
    def _cleanup_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        try:
            import gc
            gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            
            # LLMã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if hasattr(self.llm_summarizer, 'model') and hasattr(self.llm_summarizer.model, '_ctx'):
                # llama-cpp-pythonã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚»ãƒƒãƒˆ
                pass  # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
                
        except Exception as e:
            logger.debug(f"Memory cleanup warning: {e}")
    
    def _integrate_individual_summaries(
        self, 
        individual_summaries: List[Dict[str, Any]], 
        source_file: Path, 
        total_urls: int, 
        successful_count: int
    ) -> str:
        """å€‹åˆ¥è¦ç´„ã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãªè¦ç´„ã‚’ä½œæˆ"""
        
        integration_parts = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        header = f"""# Individual URL Processing Results
## Source File: {source_file}
## Processing Summary: {successful_count}/{total_urls} URLs individually processed and summarized

## Individual URL Summaries:

"""
        integration_parts.append(header)
        
        # å„å€‹åˆ¥è¦ç´„
        for i, summary_data in enumerate(individual_summaries, 1):
            individual_section = f"""
### {i}. {summary_data['name']}
- **URL**: {summary_data['url']}
- **Source**: {summary_data['source']}
- **Category**: {summary_data['category']}
- **Content Length**: {summary_data['content_length']} characters
- **Summary Type**: {summary_data['summary_type']}

**Summary:**
{summary_data['summary']}

---
"""
            integration_parts.append(individual_section)
        
        # çµ±åˆè¦ç´„ï¼ˆLLMãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.llm_summarizer and len(individual_summaries) > 1:
            try:
                # å…¨ã¦ã®å€‹åˆ¥è¦ç´„ã‚’çµ„ã¿åˆã‚ã›
                combined_summaries = "\n\n".join([
                    f"{s['name']}: {s['summary']}" for s in individual_summaries
                ])
                
                if len(combined_summaries) > 2000:
                    combined_summaries = combined_summaries[:2000] + "..."
                
                logger.info("ğŸ¤– Creating optimized integrated summary from individual summaries...")
                integrated_summary = self.llm_summarizer.summarize_english_to_english(
                    combined_summaries,
                    summary_type="brief"  # çµ±åˆè¦ç´„ã‚‚é«˜é€ŸåŒ–
                )
                
                if integrated_summary and len(integrated_summary.strip()) > 10:
                    integration_parts.append(f"""
## Integrated Summary:

{integrated_summary}

---
""")
                    logger.success("âœ… Integrated summary created from individual summaries")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to create integrated summary: {e}")
        
        # çµ±è¨ˆæƒ…å ±
        stats = f"""
## Processing Statistics:
- Total URLs found: {total_urls}
- Successfully processed: {successful_count}
- Success rate: {(successful_count/total_urls*100):.1f}% if total_urls > 0 else 0.0%
- LLM summaries: {sum(1 for s in individual_summaries if s['summary_type'] == 'llm_generated')}
- Basic summaries: {sum(1 for s in individual_summaries if s['summary_type'] == 'basic_extraction')}
"""
        integration_parts.append(stats)
        
        return "".join(integration_parts)
    
    def _extract_urls_from_json(self, json_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰URLæƒ…å ±ã‚’æŠ½å‡º"""
        urls = []
        
        # fpga_documentsæ§‹é€ ã‚’å‡¦ç†
        if 'fpga_documents' in json_data:
            for doc in json_data['fpga_documents']:
                if isinstance(doc, dict) and 'url' in doc:
                    urls.append({
                        'url': doc['url'],
                        'name': doc.get('title', doc.get('name', f"FPGA Document {len(urls)+1}")),
                        'source': 'fpga_documents',
                        'category': doc.get('category', 'FPGA Documentation')
                    })
        
        # sourcesæ§‹é€ ã‚’å‡¦ç†
        if 'sources' in json_data:
            for source_name, source_data in json_data['sources'].items():
                if isinstance(source_data, dict) and 'documents' in source_data:
                    for doc in source_data['documents']:
                        if 'url' in doc:
                            urls.append({
                                'url': doc['url'],
                                'name': doc.get('name', doc.get('title', f"Document from {source_name}")),
                                'source': source_name,
                                'category': doc.get('category', 'Document')
                            })
        
        # ç›´æ¥çš„ãªURLãƒªã‚¹ãƒˆã‚‚å‡¦ç†
        if 'urls' in json_data:
            for url_item in json_data['urls']:
                if isinstance(url_item, str):
                    urls.append({
                        'url': url_item,
                        'name': f"Document_{len(urls)+1}",
                        'source': 'direct_url',
                        'category': 'Document'
                    })
                elif isinstance(url_item, dict) and 'url' in url_item:
                    urls.append({
                        'url': url_item['url'],
                        'name': url_item.get('name', f"Document_{len(urls)+1}"),
                        'source': url_item.get('source', 'direct_url'),
                        'category': url_item.get('category', 'Document')
                    })
        
        return urls
    
    def _fetch_url_content(self, url: str) -> Optional[str]:
        """URLã‹ã‚‰å†…å®¹ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        import time
        
        for attempt in range(self.max_retries + 1):
            try:
                logger.debug(f"ğŸŒ Fetching (attempt {attempt + 1}/{self.max_retries + 1}): {url}")
                
                # ç‰¹å®šã‚µã‚¤ãƒˆå‘ã‘ã®æœ€é©åŒ–
                headers = {}
                if 'intel.com' in url:
                    headers.update({
                        'Referer': 'https://www.intel.com/',
                        'Cache-Control': 'no-cache',
                        'Pragma': 'no-cache'
                    })
                elif 'arxiv.org' in url:
                    headers.update({
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    })
                
                response = self.session.get(url, timeout=self.request_timeout, headers=headers)
                response.raise_for_status()
                
                # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                if 'text/html' in response.headers.get('content-type', ''):
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ä¸è¦ãªè¦ç´ ã‚’é™¤å»
                    for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                        element.decompose()
                    
                    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|body'))
                    
                    if main_content:
                        text = main_content.get_text(separator=' ', strip=True)
                    else:
                        text = soup.get_text(separator=' ', strip=True)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    text = re.sub(r'\s+', ' ', text)
                    text = re.sub(r'\n\s*\n', '\n\n', text)
                    
                else:
                    # ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡¦ç†
                    text = response.text
                
                # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                if len(text) > self.max_content_length:
                    text = text[:self.max_content_length] + "..."
                    logger.debug(f"âœ‚ï¸ Truncated content for {url} to {self.max_content_length} characters")
                
                return text
                
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    wait_time = (attempt + 1) * 2  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    logger.warning(f"âš ï¸ Request failed for {url} (attempt {attempt + 1}): {e}. Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.warning(f"âŒ All retry attempts failed for {url}: {e}")
                    return None
            except Exception as e:
                logger.warning(f"âš ï¸ Error processing {url}: {e}")
                return None
        
        return None  # ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
