#!/usr/bin/env python3
"""
Academic Batch Processing Integration
å­¦è¡“è«–æ–‡ãƒ»æŠ€è¡“æ–‡æ›¸ç‰¹åŒ–ã®ãƒãƒƒãƒå‡¦ç†çµ±åˆæ©Ÿèƒ½
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, Union, List
import re
from dataclasses import dataclass
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from academic.academic_processor import AcademicDocumentProcessor
from academic.academic_output_formatter import AcademicOutputFormatter
from document_processor import DocumentProcessor
from utils.language_detector import LanguageDetector


@dataclass
class AcademicBatchResult:
    """å­¦è¡“ãƒãƒƒãƒå‡¦ç†çµæœ"""
    file_path: Path
    status: str  # "success", "error", "skipped"
    summary: str
    processing_time: float
    document_type: str  # "academic_paper", "datasheet", "technical_manual", "generic"
    academic_metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class DocumentTypeClassifier:
    """æ–‡æ›¸ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ†é¡å™¨"""
    
    def __init__(self):
        # å­¦è¡“è«–æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.academic_patterns = [
            r'abstract\b.*?(introduction|background)',
            r'(methodology|methods)\b.*?results',
            r'references?\b.*?bibliography',
            r'(doi|arxiv|pubmed)',
            r'journal\s+of|proceedings\s+of|conference\s+on',
            r'(keywords?|key\s+words?)\s*:',
            r'corresponding\s+author',
            r'(manuscript|paper)\s+(received|accepted|published)'
        ]
        
        # ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
        self.datasheet_patterns = [
            r'(datasheet|data\s+sheet)',
            r'(specifications?|specs?)\b',
            r'(electrical\s+characteristics|operating\s+conditions)',
            r'(pin\s+configuration|pinout)',
            r'(absolute\s+maximum\s+ratings)',
            r'(package\s+information|ordering\s+information)',
            r'(applications?\s+circuit|typical\s+application)',
            r'(input|output)\s+(voltage|current|impedance)',
            r'(supply\s+voltage|power\s+consumption)',
            r'(temperature\s+range|operating\s+temperature)'
        ]
        
        # æŠ€è¡“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.technical_manual_patterns = [
            r'(user\s+manual|reference\s+manual|programming\s+guide)',
            r'(installation|configuration|setup)\s+(guide|instructions)',
            r'(troubleshooting|maintenance|calibration)',
            r'(safety\s+information|warnings?|cautions?)',
            r'(getting\s+started|quick\s+start)',
            r'(software\s+installation|driver\s+installation)',
            r'(command\s+reference|api\s+reference)',
            r'(version\s+history|revision\s+history)'
        ]
    
    def classify_document(self, text: str, filename: str = "") -> str:
        """
        æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡
        
        Args:
            text: æ–‡æ›¸å†…å®¹
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            æ–‡æ›¸ã‚¿ã‚¤ãƒ— ("academic_paper", "datasheet", "technical_manual", "generic")
        """
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã®æ¨å®š
        if any(keyword in filename_lower for keyword in ['datasheet', 'data_sheet', 'spec', 'specification']):
            return "datasheet"
        
        if any(keyword in filename_lower for keyword in ['manual', 'guide', 'reference', 'handbook']):
            return "technical_manual"
        
        # å†…å®¹ã‹ã‚‰ã®æ¨å®š
        academic_score = sum(1 for pattern in self.academic_patterns 
                           if re.search(pattern, text_lower, re.IGNORECASE | re.MULTILINE))
        
        datasheet_score = sum(1 for pattern in self.datasheet_patterns 
                            if re.search(pattern, text_lower, re.IGNORECASE | re.MULTILINE))
        
        manual_score = sum(1 for pattern in self.technical_manual_patterns 
                          if re.search(pattern, text_lower, re.IGNORECASE | re.MULTILINE))
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scores = {
            "academic_paper": academic_score,
            "datasheet": datasheet_score,
            "technical_manual": manual_score
        }
        
        max_score = max(scores.values())
        
        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if max_score >= 2:  # æœ€ä½2ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãŒå¿…è¦
            return max(scores, key=scores.get)
        
        return "generic"


class AcademicBatchProcessor:
    """å­¦è¡“æ–‡æ›¸ç‰¹åŒ–ãƒãƒƒãƒå‡¦ç†å™¨"""
    
    def __init__(self):
        self.academic_processor = AcademicDocumentProcessor()
        self.document_processor = DocumentProcessor()
        self.language_detector = LanguageDetector()
        self.classifier = DocumentTypeClassifier()
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥å‡¦ç†è¨­å®š
        self.type_specific_settings = {
            "academic_paper": {
                "max_length": 300,
                "focus_sections": ["abstract", "conclusion", "results"],
                "include_metadata": True,
                "detailed_analysis": True
            },
            "datasheet": {
                "max_length": 150,
                "focus_sections": ["specifications", "features", "applications"],
                "include_metadata": True,
                "detailed_analysis": False
            },
            "technical_manual": {
                "max_length": 200,
                "focus_sections": ["overview", "key_features", "usage"],
                "include_metadata": False,
                "detailed_analysis": False
            },
            "generic": {
                "max_length": 200,
                "focus_sections": [],
                "include_metadata": False,
                "detailed_analysis": False
            }
        }
    
    def process_file_specialized(
        self, 
        file_path: Path, 
        language: str = "ja",
        force_type: Optional[str] = None,
        custom_settings: Optional[Dict[str, Any]] = None
    ) -> AcademicBatchResult:
        """
        ç‰¹åŒ–å‡¦ç†ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        
        Args:
            file_path: å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            language: å‡ºåŠ›è¨€èª
            force_type: å¼·åˆ¶çš„ã«æŒ‡å®šã™ã‚‹æ–‡æ›¸ã‚¿ã‚¤ãƒ—
            custom_settings: ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
            
        Returns:
            å‡¦ç†çµæœ
        """
        start_time = datetime.now()
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = self.document_processor.process_file(file_path)
            
            if not extracted_text or len(extracted_text.strip()) < 100:
                return AcademicBatchResult(
                    file_path=file_path,
                    status="error",
                    summary="",
                    processing_time=0,
                    document_type="unknown",
                    error="ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸ååˆ†"
                )
            
            # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ†é¡
            doc_type = force_type or self.classifier.classify_document(
                extracted_text, file_path.name
            )
            
            # è¨­å®šå–å¾—
            settings = self.type_specific_settings.get(doc_type, self.type_specific_settings["generic"])
            if custom_settings:
                settings.update(custom_settings)
            
            # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥å‡¦ç†
            if doc_type == "academic_paper" and settings.get("detailed_analysis", False):
                # å­¦è¡“è«–æ–‡ã¨ã—ã¦è©³ç´°å‡¦ç†
                result = self.academic_processor.generate_academic_summary(
                    file_path,
                    language=language,
                    max_length=settings["max_length"]
                )
                
                summary = result.get("summary", "")
                academic_metadata = {
                    "structure": result.get("structure"),
                    "technical_analysis": result.get("technical_analysis"),
                    "processing_metadata": result.get("processing_metadata")
                }
                
            else:
                # ç°¡æ˜“ç‰¹åŒ–å‡¦ç†
                summary = self._generate_specialized_summary(
                    extracted_text, doc_type, settings, language
                )
                academic_metadata = {
                    "document_type": doc_type,
                    "text_length": len(extracted_text),
                    "processing_approach": "simplified_specialized"
                }
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Create result
            result = AcademicBatchResult(
                file_path=file_path,
                status="success",
                summary=summary,
                processing_time=processing_time,
                document_type=doc_type,
                academic_metadata=academic_metadata
            )
            
            # Save individual file if output directory is specified in custom_settings
            if custom_settings and custom_settings.get("output_dir"):
                self._save_individual_academic_result(result, custom_settings["output_dir"], language)
            
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return AcademicBatchResult(
                file_path=file_path,
                status="error",
                summary="",
                processing_time=processing_time,
                document_type="unknown",
                error=str(e)
            )
    
    def _generate_specialized_summary(
        self, 
        text: str, 
        doc_type: str, 
        settings: Dict[str, Any], 
        language: str
    ) -> str:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ç‰¹åŒ–ã®ç°¡æ˜“è¦ç´„ç”Ÿæˆ"""
        
        # ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–ã—ã€å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
        # é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡ºã§ã¯æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
        important_text = text[:10000]  # æœ€åˆã®10000æ–‡å­—ã‚’ä½¿ç”¨ï¼ˆååˆ†ãªæƒ…å ±ã‚’å«ã‚€ï¼‰
        
        # è¨€èªæ¤œå‡º
        detected_lang, _, _ = self.language_detector.detect_with_fallback(important_text)
        
        # ç›´æ¥å°‚é–€åŒ–è¦ç´„ã‚’ç”Ÿæˆï¼ˆç¿»è¨³æ©Ÿèƒ½ã¯ä½¿ã‚ãšã«è¦ç´„ã«é›†ä¸­ï¼‰
        summary = self._summarize_specialized_content(important_text, doc_type, settings["max_length"])
        
        # è‹±èªãƒ†ã‚­ã‚¹ãƒˆã§æ—¥æœ¬èªè¦ç´„ãŒå¿…è¦ãªå ´åˆã®ã¿ã€è¿½åŠ ç¿»è¨³ã‚’é©ç”¨
        if language == "ja" and detected_lang != "ja" and len(summary) < 100:
            # è¦ç´„ãŒçŸ­ã™ãã‚‹å ´åˆã®ã¿ç¿»è¨³æ©Ÿèƒ½ã‚’è£œåŠ©çš„ã«ä½¿ç”¨
            translated_summary = self._translate_specialized_content(important_text, doc_type)
            if len(translated_summary) > len(summary):
                summary = translated_summary
        
        return summary
    
    def _save_individual_academic_result(self, result: AcademicBatchResult, output_dir: str, language: str) -> Optional[Path]:
        """
        ã‚¢ã‚«ãƒ‡ãƒŸãƒƒã‚¯å‡¦ç†çµæœã‚’å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        
        Args:
            result: ã‚¢ã‚«ãƒ‡ãƒŸãƒƒã‚¯å‡¦ç†çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            language: å‡ºåŠ›è¨€èª
            
        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            import time
            
            output_path = Path(output_dir) / "processed"
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Generate output filename
            base_name = result.file_path.stem
            output_file = output_path / f"{base_name}_summary_{language}.md"
            
            # Create enhanced markdown content with academic metadata
            content = f"""# {result.file_path.name} - Academic Processing Result

## File Information
- **Original File**: {result.file_path.name}
- **File Size**: {result.file_path.stat().st_size / (1024 * 1024):.2f} MB
- **Processing Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **Target Language**: {language}
- **Document Type**: {result.document_type}
- **Processing Time**: {result.processing_time:.2f}s

## Summary
{result.summary}

## Academic Metadata
"""
            
            # Add academic metadata if available
            if result.academic_metadata:
                for key, value in result.academic_metadata.items():
                    if value is not None:
                        content += f"- **{key}**: {value}\n"
            
            content += "\n---\n*Generated by LocalLLM Academic Batch Processor*\n"
            
            # Write to file
            with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
                f.write(content)
            
            return output_file
            
        except Exception as e:
            print(f"Failed to save academic result for {result.file_path.name}: {e}")
            return None
    
    def _extract_focused_content(self, text: str, focus_sections: List[str], doc_type: str) -> str:
        """é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡º"""
        
        if not focus_sections:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŒ‡å®šãŒãªã„å ´åˆã¯å…ˆé ­éƒ¨åˆ†ã‚’ä½¿ç”¨
            return text[:2000]
        
        important_parts = []
        text_lower = text.lower()
        
        if doc_type == "datasheet":
            # ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã®é‡è¦éƒ¨åˆ†æŠ½å‡º
            patterns = {
                "features": r'(features?|key\s+features?|highlights?)\s*:?\s*(.*?)(?=\n\s*[A-Z]|\n\s*\d+\.|\n\s*â€¢|\Z)',
                "specifications": r'(specifications?|electrical\s+characteristics|parameters?)\s*:?\s*(.*?)(?=\n\s*[A-Z]|\n\s*\d+\.|\Z)',
                "applications": r'(applications?|typical\s+applications?|uses?)\s*:?\s*(.*?)(?=\n\s*[A-Z]|\n\s*\d+\.|\Z)'
            }
            
            for section in focus_sections:
                if section in patterns:
                    matches = re.findall(patterns[section], text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
                    for match in matches:
                        if isinstance(match, tuple):
                            important_parts.append(match[1])
                        else:
                            important_parts.append(match)
        
        elif doc_type == "academic_paper":
            # å­¦è¡“è«–æ–‡ã®é‡è¦éƒ¨åˆ†æŠ½å‡º
            patterns = {
                "abstract": r'(abstract)\s*:?\s*(.*?)(?=\n\s*(introduction|keywords?|1\.|\Z))',
                "conclusion": r'(conclusion|conclusions?)\s*:?\s*(.*?)(?=\n\s*(references?|bibliography|\Z))',
                "results": r'(results?|findings?)\s*:?\s*(.*?)(?=\n\s*(discussion|conclusion|\Z))'
            }
            
            for section in focus_sections:
                if section in patterns:
                    matches = re.findall(patterns[section], text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
                    for match in matches:
                        if isinstance(match, tuple):
                            important_parts.append(match[1])
                        else:
                            important_parts.append(match)
        
        # æŠ½å‡ºã§ããŸå ´åˆã¯ãã‚Œã‚’ã€ã§ããªã‹ã£ãŸå ´åˆã¯å…ˆé ­éƒ¨åˆ†ã‚’è¿”ã™
        if important_parts:
            combined = '\n'.join(important_parts)
            return combined[:3000]  # é©åˆ‡ãªé•·ã•ã«åˆ¶é™
        else:
            return text[:2000]
    
    def _translate_specialized_content(self, text: str, doc_type: str) -> str:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ç‰¹åŒ–ç¿»è¨³"""
        # åŸºæœ¬çš„ãªè¾æ›¸ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚’ä½¿ç”¨
        translations = {
            # ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆç”¨èª
            "specifications": "ä»•æ§˜",
            "features": "ç‰¹å¾´",
            "applications": "å¿œç”¨",
            "operating conditions": "å‹•ä½œæ¡ä»¶",
            "electrical characteristics": "é›»æ°—çš„ç‰¹æ€§",
            "pin configuration": "ãƒ”ãƒ³é…ç½®",
            "package information": "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æƒ…å ±",
            "absolute maximum ratings": "çµ¶å¯¾æœ€å¤§å®šæ ¼",
            
            # å­¦è¡“è«–æ–‡ç”¨èª
            "abstract": "è¦ç´„",
            "introduction": "ã¯ã˜ã‚ã«",
            "methodology": "æ‰‹æ³•",
            "results": "çµæœ",
            "discussion": "è€ƒå¯Ÿ",
            "conclusion": "çµè«–",
            "references": "å‚è€ƒæ–‡çŒ®",
            "keywords": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            
            # æŠ€è¡“æ–‡æ›¸å…±é€šç”¨èª
            "overview": "æ¦‚è¦",
            "description": "èª¬æ˜",
            "performance": "æ€§èƒ½",
            "efficiency": "åŠ¹ç‡",
            "accuracy": "ç²¾åº¦",
            "reliability": "ä¿¡é ¼æ€§"
        }
        
        # ç°¡æ˜“ç¿»è¨³ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼‰
        translated = text
        for en, ja in translations.items():
            translated = re.sub(r'\b' + re.escape(en) + r'\b', ja, translated, flags=re.IGNORECASE)
        
        return translated[:1000]  # é©åˆ‡ãªé•·ã•ã«åˆ¶é™
    
    def _summarize_specialized_content(self, text: str, doc_type: str, max_length: int) -> str:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ç‰¹åŒ–è¦ç´„"""
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥ã®è¦ç´„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        if doc_type == "datasheet":
            summary_parts = ["ã€ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆè¦ç´„ã€‘"]
            
            # æ”¹è¡Œæ–‡å­—ã§æ­£ã—ãåˆ†å‰²
            lines = [line.strip() for line in re.split(r'[\n\r]+', text) if line.strip() and len(line.strip()) > 3]
            
            # AD637ç‰¹æœ‰ã®æŠ€è¡“ä»•æ§˜ã‚’æ¤œç´¢
            specs = []
            features = []
            
            for line in lines:
                lower_line = line.lower()
                # ç²¾åº¦ã€é›»åœ§ã€å‘¨æ³¢æ•°ãªã©ã®ä»•æ§˜
                if any(keyword in lower_line for keyword in ['ç²¾åº¦', 'accuracy', 'é›»åœ§', 'voltage', 'å‘¨æ³¢æ•°', 'frequency', 'rms', 'éç›´ç·šæ€§', 'ã‚¯ãƒ¬ã‚¹ãƒˆ', 'crest']):
                    if line not in specs and len(line) > 10:
                        specs.append(line)
                # ç‰¹é•·ãƒ»æ©Ÿèƒ½
                elif any(keyword in lower_line for keyword in ['ç‰¹é•·', 'feature', 'æ©Ÿèƒ½', 'function', 'è¨ˆç®—', 'calculation', 'ãƒ‘ãƒ¯ãƒ¼', 'power']):
                    if line not in features and len(line) > 10:
                        features.append(line)
                        
            # ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            if specs:
                summary_parts.append("\nã€ä¸»è¦ä»•æ§˜ã€‘")
                summary_parts.extend([f"â€¢ {spec}" for spec in specs[:5]])
                
            # ç‰¹é•·ã‚»ã‚¯ã‚·ãƒ§ãƒ³  
            if features:
                summary_parts.append("\nã€ä¸»è¦ç‰¹é•·ã€‘")
                summary_parts.extend([f"â€¢ {feature}" for feature in features[:5]])
                
            # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
            if not specs and not features:
                important_lines = []
                for line in lines:
                    if (len(line) > 20 and 
                        any(keyword in line for keyword in ['AD637', 'RMS', 'é«˜ç²¾åº¦', 'åºƒå¸¯åŸŸ', 'ã‚³ãƒ³ãƒãƒ¼ã‚¿', 'dB', 'ãƒãƒƒãƒ—', 'ãƒ”ãƒ³'])):
                        important_lines.append(line)
                        
                if important_lines:
                    summary_parts.append("\nã€æŠ€è¡“æƒ…å ±ã€‘")
                    summary_parts.extend([f"â€¢ {line}" for line in important_lines[:6]])
            
            summary_template = '\n'.join(summary_parts)
            
        elif doc_type == "academic_paper":
            summary_template = "ã€è«–æ–‡è¦ç´„ã€‘\n"
            # é‡è¦ãªæ–‡ã‚’æŠ½å‡ºï¼ˆé•·ã„æ–‡ã€æ•°å€¤ã‚’å«ã‚€æ–‡ã‚’å„ªå…ˆï¼‰
            sentences = re.split(r'[.!?]+', text)
            important_sentences = sorted(sentences, key=lambda x: len(x) + x.count('%') * 10 + x.count('result') * 5, reverse=True)
            summary_template += ' '.join(important_sentences[:3])
            
        elif doc_type == "technical_manual":
            summary_template = "ã€æŠ€è¡“æ–‡æ›¸è¦ç´„ã€‘\n"
            # æ‰‹é †ã‚„é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
            lines = re.split(r'[\n\r]+', text)
            procedure_lines = [line for line in lines if any(keyword in line.lower() 
                                                           for keyword in ['step', 'procedure', 'important', 'note', 'warning', 'install', 'configure'])]
            summary_template += '\n'.join(procedure_lines[:4])
            
        else:
            summary_template = "ã€æ–‡æ›¸è¦ç´„ã€‘\n"
            summary_template += text[:max_length]
        
        # æœ€å¤§é•·ã«åˆ¶é™
        if len(summary_template) > max_length:
            summary_template = summary_template[:max_length] + "..."
        
        return summary_template


def create_academic_batch_processing_function(
    language: str = "ja",
    force_type: Optional[str] = None,
    custom_settings: Optional[Dict[str, Any]] = None,
    use_comprehensive_format: bool = True,
    include_metadata: bool = False
):
    """
    ãƒãƒƒãƒå‡¦ç†ç”¨ã®å­¦è¡“ç‰¹åŒ–å‡¦ç†é–¢æ•°ã‚’ä½œæˆ
    
    Args:
        language: å‡ºåŠ›è¨€èª
        force_type: å¼·åˆ¶çš„ã«æŒ‡å®šã™ã‚‹æ–‡æ›¸ã‚¿ã‚¤ãƒ—
        custom_settings: ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
        use_comprehensive_format: åŒ…æ‹¬çš„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‹
        include_metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹
        
    Returns:
        ãƒãƒƒãƒå‡¦ç†ã§ä½¿ç”¨å¯èƒ½ãªå‡¦ç†é–¢æ•°
    """
    processor = AcademicBatchProcessor()
    formatter = AcademicOutputFormatter()
    
    def academic_batch_process_function(file_path: Path, **kwargs) -> AcademicBatchResult:
        """ãƒãƒƒãƒå‡¦ç†ç”¨é–¢æ•°ï¼ˆåŒ…æ‹¬çš„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼‰"""
        # åŸºæœ¬å‡¦ç†
        result = processor.process_file_specialized(
            file_path=file_path,
            language=language,
            force_type=force_type,
            custom_settings=custom_settings
        )
        
        # åŒ…æ‹¬çš„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒæœ‰åŠ¹ãªå ´åˆã€ã‚µãƒãƒªãƒ¼ã‚’å†ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        if use_comprehensive_format and result.status == "success":
            try:
                # ã‚¢ã‚«ãƒ‡ãƒŸãƒƒã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ§‹é€ åŒ–ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
                if result.academic_metadata:
                    structured_summary = formatter.create_structured_summary(result.academic_metadata)
                    comprehensive_summary = formatter.format_comprehensive_summary(
                        structured_summary, 
                        include_metadata=include_metadata
                    )
                    # å…ƒã®ã‚µãƒãƒªãƒ¼ã‚’åŒ…æ‹¬çš„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ç½®ãæ›ãˆ
                    result.summary = comprehensive_summary
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤±æ•—æ™‚ã¯å…ƒã®ã‚µãƒãƒªãƒ¼ã‚’ä¿æŒ
                print(f"ğŸ“‹ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return result
    
    return academic_batch_process_function


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    processor = AcademicBatchProcessor()
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
    test_files = list(Path("data").glob("*.pdf"))
    if test_files:
        print("ğŸ§ª å­¦è¡“ç‰¹åŒ–ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ")
        result = processor.process_file_specialized(test_files[0])
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {result.file_path.name}")
        print(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {result.document_type}")
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.status}")
        print(f"è¦ç´„: {result.summary[:200]}...")
    else:
        print("ğŸ“ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
