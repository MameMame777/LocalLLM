"""
JSON URL Processor for LocalLLM
JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã€å„URLã®å†…å®¹ã‚’è¦ç´„ã™ã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
"""

import json
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
import time
import re
from bs4 import BeautifulSoup
from loguru import logger


class JSONUrlProcessor:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã¦è¦ç´„å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.max_content_length = 50000  # æœ€å¤§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·
        self.request_timeout = 30  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        
    def process_json_file(self, json_file_path: Path) -> Dict[str, Any]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦URLä¸€è¦§ã‚’æŠ½å‡ºã—ã€å„URLã®å†…å®¹ã‚’å–å¾—"""
        try:
            logger.info(f"ğŸ“„ Processing JSON file: {json_file_path}")
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(json_file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # URLã‚’æŠ½å‡º
            urls = self._extract_urls_from_json(json_data)
            logger.info(f"ğŸ”— Found {len(urls)} URLs to process")
            
            # å„URLã®å†…å®¹ã‚’å–å¾—
            url_contents = []
            for i, url_info in enumerate(urls, 1):
                logger.info(f"ğŸŒ Processing URL {i}/{len(urls)}: {url_info['url']}")
                
                content = self._fetch_url_content(url_info['url'])
                if content:
                    url_contents.append({
                        'name': url_info.get('name', f"Document_{i}"),
                        'url': url_info['url'],
                        'source': url_info.get('source', 'unknown'),
                        'category': url_info.get('category', 'Document'),
                        'content': content,
                        'content_length': len(content)
                    })
                    logger.success(f"âœ… Successfully fetched: {url_info.get('name', 'Document')} ({len(content)} chars)")
                else:
                    logger.warning(f"âš ï¸ Failed to fetch: {url_info['url']}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
                time.sleep(1)
            
            return {
                'source_file': str(json_file_path),
                'total_urls': len(urls),
                'successful_fetches': len(url_contents),
                'url_contents': url_contents,
                'metadata': json_data.get('scan_info', {})
            }
            
        except Exception as e:
            logger.error(f"âŒ Error processing JSON file: {e}")
            raise
    
    def _extract_urls_from_json(self, json_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰URLæƒ…å ±ã‚’æŠ½å‡º"""
        urls = []
        
        # sourcesæ§‹é€ ã‚’å‡¦ç†
        if 'sources' in json_data:
            for source_name, source_data in json_data['sources'].items():
                if 'documents' in source_data:
                    for doc in source_data['documents']:
                        if 'url' in doc:
                            urls.append({
                                'name': doc.get('name', 'Unknown Document'),
                                'url': doc['url'],
                                'source': source_name,
                                'category': doc.get('category', 'Document'),
                                'file_type': doc.get('file_type', 'html')
                            })
        
        # ãã®ä»–ã®æ§‹é€ ã‚‚å‡¦ç†ï¼ˆæŸ”è»Ÿæ€§ã®ãŸã‚ï¼‰
        urls.extend(self._recursive_url_search(json_data))
        
        # é‡è¤‡ã‚’é™¤å»
        seen_urls = set()
        unique_urls = []
        for url_info in urls:
            if url_info['url'] not in seen_urls:
                seen_urls.add(url_info['url'])
                unique_urls.append(url_info)
        
        return unique_urls
    
    def _recursive_url_search(self, data: Any, urls: List[Dict[str, str]] = None) -> List[Dict[str, str]]:
        """å†å¸°çš„ã«URLæ–‡å­—åˆ—ã‚’æ¤œç´¢"""
        if urls is None:
            urls = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                if key == 'url' and isinstance(value, str) and value.startswith('http'):
                    # æ—¢ã«è¿½åŠ æ¸ˆã¿ã§ãªã‘ã‚Œã°è¿½åŠ 
                    if not any(u['url'] == value for u in urls):
                        urls.append({
                            'name': data.get('name', 'Extracted URL'),
                            'url': value,
                            'source': data.get('source', 'extracted'),
                            'category': data.get('category', 'Document'),
                            'file_type': data.get('file_type', 'html')
                        })
                else:
                    self._recursive_url_search(value, urls)
        elif isinstance(data, list):
            for item in data:
                self._recursive_url_search(item, urls)
        
        return urls
    
    def _fetch_url_content(self, url: str) -> Optional[str]:
        """URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        try:
            logger.debug(f"ğŸŒ Fetching: {url}")
            
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            # Content-Typeã‚’ãƒã‚§ãƒƒã‚¯
            content_type = response.headers.get('content-type', '').lower()
            
            if 'text/html' in content_type:
                return self._extract_text_from_html(response.text, url)
            elif 'application/pdf' in content_type:
                logger.warning(f"âš ï¸ PDF content detected for {url}, skipping (PDF processing requires separate handler)")
                return None
            elif 'text/' in content_type:
                return response.text[:self.max_content_length]
            else:
                logger.warning(f"âš ï¸ Unsupported content type: {content_type} for {url}")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ Request failed for {url}: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Error processing {url}: {e}")
            return None
    
    def _extract_text_from_html(self, html_content: str, url: str) -> str:
        """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ä¸è¦ãªè¦ç´ ã‚’é™¤å»
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|body'))
            
            if main_content:
                text = main_content.get_text(separator=' ', strip=True)
            else:
                text = soup.get_text(separator=' ', strip=True)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            text = re.sub(r'\s+', ' ', text)  # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
            text = re.sub(r'\n\s*\n', '\n\n', text)  # è¤‡æ•°ã®æ”¹è¡Œã‚’æ•´ç†
            
            # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            if len(text) > self.max_content_length:
                text = text[:self.max_content_length] + "..."
                logger.debug(f"âœ‚ï¸ Truncated content for {url} to {self.max_content_length} characters")
            
            return text
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error extracting text from HTML for {url}: {e}")
            return ""
    
    def create_summary_content(self, url_data: Dict[str, Any]) -> str:
        """URLå‡¦ç†çµæœã‹ã‚‰è¦ç´„ç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ"""
        
        summary_parts = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        header = f"""# JSON URL Processing Results
## Source File: {url_data['source_file']}
## Processing Summary: {url_data['successful_fetches']}/{url_data['total_urls']} URLs successfully processed

"""
        summary_parts.append(header)
        
        # å„URLã®å†…å®¹
        for i, url_content in enumerate(url_data['url_contents'], 1):
            url_section = f"""
## Document {i}: {url_content['name']}
**Source:** {url_content['source']}
**Category:** {url_content['category']}
**URL:** {url_content['url']}
**Content Length:** {url_content['content_length']} characters

### Content:
{url_content['content']}

---
"""
            summary_parts.append(url_section)
        
        combined_content = "\n".join(summary_parts)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        if url_data.get('metadata'):
            metadata_section = f"""
## Source Metadata
```json
{json.dumps(url_data['metadata'], indent=2, ensure_ascii=False)}
```
"""
            combined_content += metadata_section
        
        return combined_content


# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_json_url_processor():
    """JSON URL ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    json_file = Path("e:/Nautilus/Desktop/fpga_documents.json")
    
    if not json_file.exists():
        logger.error(f"âŒ Test JSON file not found: {json_file}")
        return False
    
    try:
        processor = JSONUrlProcessor()
        
        logger.info("ğŸ§ª Testing JSON URL Processor")
        logger.info(f"ğŸ“„ Processing: {json_file}")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        result = processor.process_json_file(json_file)
        
        # è¦ç´„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
        summary_content = processor.create_summary_content(result)
        
        # çµæœã‚’å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_dir = Path("output/json_url_test")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"fpga_documents_processed_{int(time.time())}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        logger.success(f"âœ… JSON URL processing completed!")
        logger.info(f"ğŸ“Š Results:")
        logger.info(f"   ğŸ“ Source: {result['source_file']}")
        logger.info(f"   ğŸ”— Total URLs: {result['total_urls']}")
        logger.info(f"   âœ… Successful: {result['successful_fetches']}")
        logger.info(f"   ğŸ“„ Output: {output_file}")
        logger.info(f"   ğŸ“ Content Length: {len(summary_content)} characters")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ JSON URL processing test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_json_url_processor()
