#!/usr/bin/env python3
"""
Academic Paper Processing Engine
Specialized processing for academic papers and technical documents
"""

import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from loguru import logger
import json

# Add project paths
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.insert(0, str(Path(__file__).parent.parent))

from document_processor import DocumentProcessor
from batch_processing.batch_processor import ProcessingResult
from academic.technical_translator import TechnicalDocumentTranslator, TechnicalTranslationResult
from academic.academic_output_formatter import AcademicOutputFormatter

@dataclass
class AcademicStructure:
    """å­¦è¡“è«–æ–‡ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿"""
    title: Optional[str] = None
    authors: List[str] = None
    abstract: Optional[str] = None
    keywords: List[str] = None
    introduction: Optional[str] = None
    methodology: Optional[str] = None
    results: Optional[str] = None
    discussion: Optional[str] = None
    conclusion: Optional[str] = None
    references: List[str] = None
    figures: List[str] = None
    tables: List[str] = None
    equations: List[str] = None

@dataclass
class TechnicalSummary:
    """æŠ€è¡“æ–‡æ›¸è¦ç´„çµæœ"""
    document_type: str  # "research_paper", "technical_report", "patent", "manual"
    technical_level: str  # "basic", "intermediate", "advanced", "expert"
    main_contribution: str
    key_findings: List[str]
    technical_details: List[str]
    practical_applications: List[str]
    limitations: List[str]
    future_work: Optional[str]
    methodology_summary: Optional[str]
    mathematical_concepts: List[str]

class AcademicDocumentProcessor:
    """å­¦è¡“ãƒ»æŠ€è¡“æ–‡æ›¸å°‚ç”¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    
    def __init__(self, llm_processor=None):
        self.base_processor = DocumentProcessor()
        self.formatter = AcademicOutputFormatter()
        self.technical_translator = TechnicalDocumentTranslator(llm_processor)
        self.llm_processor = llm_processor
        
        # å­¦è¡“è«–æ–‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.section_patterns = {
            'title': [
                r'^[\s]*(?:Title|ã‚¿ã‚¤ãƒˆãƒ«)[\s]*:?[\s]*(.+?)(?:\n|$)',
                r'^[\s]*(.+?)(?:\n.*?Abstract|\n.*?è¦ç´„)',
                r'^\s*([A-Z][^.\n]{10,100})\s*$'
            ],
            'authors': [
                r'(?:Authors?|è‘—è€…)[\s]*:?[\s]*(.+?)(?:\n|Abstract)',
                r'(?:By|åŸ·ç­†è€…)[\s]*:?[\s]*(.+?)(?:\n)',
                r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:\s*,\s*[A-Z][a-z]+\s+[A-Z][a-z]+)*)'
            ],
            'abstract': [
                r'(?:Abstract|è¦ç´„|æ¦‚è¦)[\s]*:?[\s]*\n?(.*?)(?:\n\s*(?:Keywords?|ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|Introduction|1\.|I\.)|$)',
                r'(?:ABSTRACT|æŠ„éŒ²)[\s]*\n?(.*?)(?:\n\s*(?:Keywords?|ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|Introduction)|$)'
            ],
            'keywords': [
                r'(?:Keywords?|ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰|Key\s*words?)[\s]*:?[\s]*(.+?)(?:\n\s*\n|\n\s*(?:Introduction|1\.))',
                r'(?:Index\s*terms?|ç´¢å¼•èª)[\s]*:?[\s]*(.+?)(?:\n)'
            ],
            'introduction': [
                r'(?:1\.|I\.|Introduction|ã¯ã˜ã‚ã«|åºè«–)[\s]*(?:Introduction)?[\s]*\n(.*?)(?:\n\s*(?:2\.|II\.|Methodology|Method|æ‰‹æ³•))',
                r'(?:Introduction|åºè«–|ã¯ã˜ã‚ã«)[\s]*\n(.*?)(?:\n\s*(?:Methodology|Method|Related Work))'
            ],
            'methodology': [
                r'(?:2\.|II\.|Methodology|Method|æ‰‹æ³•|æ–¹æ³•è«–)[\s]*(?:Methodology|Method)?[\s]*\n(.*?)(?:\n\s*(?:3\.|III\.|Results?|å®Ÿé¨“|çµæœ))',
                r'(?:Methodology|Method|æ‰‹æ³•|æ–¹æ³•è«–|å®Ÿé¨“æ–¹æ³•)[\s]*\n(.*?)(?:\n\s*(?:Results?|å®Ÿé¨“çµæœ|çµæœ))'
            ],
            'results': [
                r'(?:3\.|III\.|Results?|å®Ÿé¨“çµæœ|çµæœ)[\s]*(?:Results?)?[\s]*\n(.*?)(?:\n\s*(?:4\.|IV\.|Discussion|è€ƒå¯Ÿ|è­°è«–))',
                r'(?:Results?|å®Ÿé¨“çµæœ|çµæœ|æˆæœ)[\s]*\n(.*?)(?:\n\s*(?:Discussion|è€ƒå¯Ÿ))'
            ],
            'discussion': [
                r'(?:4\.|IV\.|Discussion|è€ƒå¯Ÿ|è­°è«–)[\s]*(?:Discussion)?[\s]*\n(.*?)(?:\n\s*(?:5\.|V\.|Conclusion|çµè«–))',
                r'(?:Discussion|è€ƒå¯Ÿ|è­°è«–|æ¤œè¨)[\s]*\n(.*?)(?:\n\s*(?:Conclusion|çµè«–))'
            ],
            'conclusion': [
                r'(?:5\.|V\.|Conclusion|çµè«–|ã¾ã¨ã‚)[\s]*(?:Conclusion|ã¾ã¨ã‚)?[\s]*\n(.*?)(?:\n\s*(?:References?|å‚è€ƒæ–‡çŒ®|Acknowledgment))',
                r'(?:Conclusion|çµè«–|ã¾ã¨ã‚|ãŠã‚ã‚Šã«)[\s]*\n(.*?)(?:\n\s*(?:References?|å‚è€ƒæ–‡çŒ®))'
            ],
            'references': [
                r'(?:References?|å‚è€ƒæ–‡çŒ®|Bibliography|æ–‡çŒ®)[\s]*\n(.*?)(?:\n\s*(?:Appendix|ä»˜éŒ²)|$)',
                r'(?:REFERENCES|å‚è€ƒæ–‡çŒ®)[\s]*\n(.*?)$'
            ]
        }
        
        # æŠ€è¡“ç”¨èªãƒ»æ¦‚å¿µèªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.technical_patterns = {
            'mathematical_concepts': [
                r'(?:theorem|å®šç†|lemma|è£œé¡Œ|corollary|ç³»|proof|è¨¼æ˜)',
                r'(?:algorithm|ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ |optimization|æœ€é©åŒ–|convergence|åæŸ)',
                r'(?:matrix|è¡Œåˆ—|vector|ãƒ™ã‚¯ãƒˆãƒ«|eigenvalue|å›ºæœ‰å€¤|gradient|å‹¾é…)',
                r'(?:neural network|ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯|deep learning|æ·±å±¤å­¦ç¿’)',
                r'(?:machine learning|æ©Ÿæ¢°å­¦ç¿’|artificial intelligence|äººå·¥çŸ¥èƒ½)'
            ],
            'research_methods': [
                r'(?:experiment|å®Ÿé¨“|simulation|ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³|analysis|è§£æ)',
                r'(?:survey|èª¿æŸ»|interview|ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼|questionnaire|ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ)',
                r'(?:statistical|çµ±è¨ˆçš„|quantitative|å®šé‡çš„|qualitative|å®šæ€§çš„)',
                r'(?:cross-validation|äº¤å·®æ¤œè¨¼|hypothesis|ä»®èª¬|significance|æœ‰æ„æ€§)'
            ],
            'technical_metrics': [
                r'(?:accuracy|ç²¾åº¦|precision|é©åˆç‡|recall|å†ç¾ç‡|F1-score)',
                r'(?:RMSE|MAE|MSE|R-squared|AUC|ROC)',
                r'(?:throughput|ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ|latency|ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·|bandwidth|å¸¯åŸŸå¹…)',
                r'(?:efficiency|åŠ¹ç‡|performance|æ€§èƒ½|scalability|æ‹¡å¼µæ€§)'
            ]
        }

    def extract_academic_structure(self, text: str) -> AcademicStructure:
        """å­¦è¡“è«–æ–‡ã®æ§‹é€ ã‚’æŠ½å‡º"""
        structure = AcademicStructure()
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æŠ½å‡º
        for section, patterns in self.section_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                if match:
                    content = match.group(1).strip()
                    if len(content) > 10:  # æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿
                        if section == 'authors':
                            # è‘—è€…åã‚’åˆ†é›¢
                            authors = re.split(r'[,;]\s*|\s+and\s+', content)
                            structure.authors = [author.strip() for author in authors if author.strip()]
                        elif section == 'keywords':
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†é›¢
                            keywords = re.split(r'[,;]\s*', content)
                            structure.keywords = [kw.strip() for kw in keywords if kw.strip()]
                        elif section == 'references':
                            # å‚è€ƒæ–‡çŒ®ã‚’åˆ†é›¢
                            refs = re.split(r'\n\s*\[\d+\]|\n\s*\d+\.', content)
                            structure.references = [ref.strip() for ref in refs if ref.strip()]
                        else:
                            setattr(structure, section, content[:2000])  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                        break
        
        # å›³è¡¨ãƒ»æ•°å¼ã®æŠ½å‡º
        structure.figures = self._extract_figures(text)
        structure.tables = self._extract_tables(text)
        structure.equations = self._extract_equations(text)
        
        return structure

    def _extract_figures(self, text: str) -> List[str]:
        """å›³è¡¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        figure_patterns = [
            r'(?:Figure|Fig\.|å›³|Figure)\s*(\d+)[\s:]*(.{10,200}?)(?:\n|$)',
            r'(?:Figure|å›³)\s*(\d+)[\s]*:[\s]*(.+?)(?:\n)',
        ]
        
        figures = []
        for pattern in figure_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) >= 2:
                    figures.append(f"Figure {match[0]}: {match[1].strip()}")
        
        return figures[:10]  # æœ€å¤§10å€‹

    def _extract_tables(self, text: str) -> List[str]:
        """è¡¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        table_patterns = [
            r'(?:Table|è¡¨)\s*(\d+)[\s:]*(.{10,200}?)(?:\n|$)',
            r'(?:Table|è¡¨)\s*(\d+)[\s]*:[\s]*(.+?)(?:\n)',
        ]
        
        tables = []
        for pattern in table_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) >= 2:
                    tables.append(f"Table {match[0]}: {match[1].strip()}")
        
        return tables[:10]  # æœ€å¤§10å€‹

    def _extract_equations(self, text: str) -> List[str]:
        """æ•°å¼ã‚’æŠ½å‡º"""
        equation_patterns = [
            r'\$\$(.+?)\$\$',  # LaTeX display math
            r'\$(.+?)\$',      # LaTeX inline math
            r'\\begin\{equation\}(.+?)\\end\{equation\}',  # LaTeX equation
            r'\\begin\{align\}(.+?)\\end\{align\}',        # LaTeX align
        ]
        
        equations = []
        for pattern in equation_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                clean_eq = match.strip()
                if len(clean_eq) > 3:
                    equations.append(clean_eq)
        
        return equations[:20]  # æœ€å¤§20å€‹

    def analyze_technical_content(self, text: str, structure: AcademicStructure) -> TechnicalSummary:
        """æŠ€è¡“å†…å®¹ã®åˆ†æ"""
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        doc_type = self._classify_document_type(text, structure)
        
        # æŠ€è¡“ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
        tech_level = self._assess_technical_level(text)
        
        # ä¸»è¦è²¢çŒ®ã®æŠ½å‡º
        main_contribution = self._extract_main_contribution(structure)
        
        # ä¸»è¦ç™ºè¦‹ã®æŠ½å‡º
        key_findings = self._extract_key_findings(structure)
        
        # æŠ€è¡“è©³ç´°ã®æŠ½å‡º
        technical_details = self._extract_technical_details(text)
        
        # å®Ÿç”¨çš„å¿œç”¨ã®æŠ½å‡º
        practical_applications = self._extract_applications(text)
        
        # åˆ¶é™äº‹é …ã®æŠ½å‡º
        limitations = self._extract_limitations(text)
        
        # ä»Šå¾Œã®èª²é¡Œ
        future_work = self._extract_future_work(structure)
        
        # æ‰‹æ³•è¦ç´„
        methodology_summary = structure.methodology[:500] if structure.methodology else None
        
        # æ•°å­¦çš„æ¦‚å¿µ
        mathematical_concepts = self._extract_mathematical_concepts(text)
        
        return TechnicalSummary(
            document_type=doc_type,
            technical_level=tech_level,
            main_contribution=main_contribution,
            key_findings=key_findings,
            technical_details=technical_details,
            practical_applications=practical_applications,
            limitations=limitations,
            future_work=future_work,
            methodology_summary=methodology_summary,
            mathematical_concepts=mathematical_concepts
        )

    def _classify_document_type(self, text: str, structure: AcademicStructure) -> str:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['patent', 'ç‰¹è¨±', 'invention', 'claim']):
            return "patent"
        elif any(word in text_lower for word in ['manual', 'user guide', 'tutorial', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«', 'å–æ‰±èª¬æ˜æ›¸']):
            return "manual"
        elif any(word in text_lower for word in ['technical report', 'æŠ€è¡“å ±å‘Š', 'white paper']):
            return "technical_report"
        elif structure.abstract or structure.references:
            return "research_paper"
        else:
            return "technical_document"

    def _assess_technical_level(self, text: str) -> str:
        """æŠ€è¡“ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡"""
        # å°‚é–€ç”¨èªã®å¯†åº¦ã‚’è¨ˆç®—
        technical_terms = 0
        basic_terms = 0
        
        advanced_patterns = [
            r'(?:algorithm|optimization|convergence|eigenvalue|gradient)',
            r'(?:neural network|deep learning|machine learning)',
            r'(?:statistical significance|hypothesis testing|p-value)',
            r'(?:complexity analysis|computational complexity)',
            r'(?:stochastic|probabilistic|Bayesian|regression)'
        ]
        
        basic_patterns = [
            r'(?:data|information|system|method|result)',
            r'(?:analysis|comparison|evaluation|measurement)',
            r'(?:performance|efficiency|accuracy|speed)'
        ]
        
        for pattern in advanced_patterns:
            technical_terms += len(re.findall(pattern, text, re.IGNORECASE))
        
        for pattern in basic_patterns:
            basic_terms += len(re.findall(pattern, text, re.IGNORECASE))
        
        total_words = len(text.split())
        advanced_ratio = technical_terms / total_words if total_words > 0 else 0
        
        if advanced_ratio > 0.02:
            return "expert"
        elif advanced_ratio > 0.01:
            return "advanced"
        elif advanced_ratio > 0.005:
            return "intermediate"
        else:
            return "basic"

    def _extract_main_contribution(self, structure: AcademicStructure) -> str:
        """ä¸»è¦è²¢çŒ®ã®æŠ½å‡º"""
        candidates = []
        
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‹ã‚‰
        if structure.abstract:
            # è²¢çŒ®ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã‚’æŠ½å‡º
            contribution_patterns = [
                r'(?:we propose|we present|we introduce|we develop|this paper presents|our contribution|æˆ‘ã€…ã¯ææ¡ˆ|æœ¬è«–æ–‡ã§ã¯|ææ¡ˆã™ã‚‹)',
                r'(?:novel|new|innovative|original|improved|enhanced|æ–°ã—ã„|æ–°è¦|æ”¹è‰¯|å‘ä¸Š)'
            ]
            
            for pattern in contribution_patterns:
                matches = re.finditer(pattern, structure.abstract, re.IGNORECASE)
                for match in matches:
                    # ãƒãƒƒãƒå‘¨è¾ºã®æ–‡ã‚’æŠ½å‡º
                    start = max(0, match.start() - 50)
                    end = min(len(structure.abstract), match.end() + 150)
                    candidates.append(structure.abstract[start:end].strip())
        
        # çµè«–ã‹ã‚‰
        if structure.conclusion:
            conclusion_sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', structure.conclusion)
            candidates.extend([s.strip() for s in conclusion_sentences[:3] if len(s.strip()) > 20])
        
        if candidates:
            # æœ€ã‚‚é•·ã„å€™è£œã‚’é¸æŠ
            return max(candidates, key=len)[:300]
        
        return "ä¸»è¦è²¢çŒ®ã®ç‰¹å®šãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    def _extract_key_findings(self, structure: AcademicStructure) -> List[str]:
        """ä¸»è¦ç™ºè¦‹ã®æŠ½å‡º"""
        findings = []
        
        # çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰
        if structure.results:
            result_sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', structure.results)
            for sentence in result_sentences:
                if len(sentence.strip()) > 30 and any(word in sentence.lower() for word in 
                    ['achieved', 'improved', 'increased', 'decreased', 'demonstrated', 'showed', 'found',
                     'é”æˆ', 'æ”¹å–„', 'å‘ä¸Š', 'æ¸›å°‘', 'ç¤ºã—ãŸ', 'ç™ºè¦‹', 'æ˜ã‚‰ã‹ã«']):
                    findings.append(sentence.strip())
        
        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‹ã‚‰æ•°å€¤çš„çµæœ
        if structure.abstract:
            number_patterns = [
                r'(\d+(?:\.\d+)?%\s*(?:improvement|increase|decrease|accuracy|precision|æ”¹å–„|å‘ä¸Š|ç²¾åº¦))',
                r'(\d+(?:\.\d+)?\s*(?:times faster|å€é«˜é€Ÿ|å€ã®æ€§èƒ½))',
                r'(\d+(?:\.\d+)?\s*(?:dB|Hz|MHz|GHz|ms|Î¼s|ns))'
            ]
            
            for pattern in number_patterns:
                matches = re.findall(pattern, structure.abstract, re.IGNORECASE)
                findings.extend(matches)
        
        return findings[:10]

    def _extract_technical_details(self, text: str) -> List[str]:
        """æŠ€è¡“è©³ç´°ã®æŠ½å‡º"""
        details = []
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ»æ‰‹æ³•ã®è©³ç´°
        algorithm_patterns = [
            r'(?:Algorithm|ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )\s*\d*[\s:]*(.{50,300}?)(?:\n|Algorithm|\d\.)',
            r'(?:Method|æ‰‹æ³•|æ–¹æ³•)[\s:]*(.{50,300}?)(?:\n\n|\d\.)',
            r'(?:Implementation|å®Ÿè£…|å®Ÿè£…æ–¹æ³•)[\s:]*(.{50,300}?)(?:\n\n|\d\.)'
        ]
        
        for pattern in algorithm_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            details.extend([match.strip() for match in matches if len(match.strip()) > 20])
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        parameter_patterns = [
            r'(?:parameters?|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿|è¨­å®š)[\s:]*(.{30,200}?)(?:\n\n|\d\.)',
            r'(?:hyperparameters?|ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)[\s:]*(.{30,200}?)(?:\n\n|\d\.)'
        ]
        
        for pattern in parameter_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            details.extend([match.strip() for match in matches if len(match.strip()) > 15])
        
        return details[:8]

    def _extract_applications(self, text: str) -> List[str]:
        """å®Ÿç”¨çš„å¿œç”¨ã®æŠ½å‡º"""
        applications = []
        
        application_patterns = [
            r'(?:applications?|å¿œç”¨|é©ç”¨)[\s:]*(.{50,300}?)(?:\n\n|\d\.)',
            r'(?:use case|ä½¿ç”¨ä¾‹|ç”¨é€”)[\s:]*(.{50,300}?)(?:\n\n|\d\.)',
            r'(?:practical|å®Ÿç”¨çš„|å®Ÿéš›ã®)[\s\w]*(?:application|å¿œç”¨|é©ç”¨)[\s:]*(.{50,300}?)(?:\n\n|\d\.)'
        ]
        
        for pattern in application_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            applications.extend([match.strip() for match in matches if len(match.strip()) > 20])
        
        return applications[:5]

    def _extract_limitations(self, text: str) -> List[str]:
        """åˆ¶é™äº‹é …ã®æŠ½å‡º"""
        limitations = []
        
        limitation_patterns = [
            r'(?:limitations?|åˆ¶é™|é™ç•Œ)[\s:]*(.{50,300}?)(?:\n\n|\d\.)',
            r'(?:however|ä½†ã—|ã—ã‹ã—)[\s,]*(.{50,300}?)(?:\n\n|\d\.)',
            r'(?:cannot|can not|ã§ããªã„|ä¸å¯èƒ½)(.{30,200}?)(?:\n\n|\d\.)'
        ]
        
        for pattern in limitation_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            limitations.extend([match.strip() for match in matches if len(match.strip()) > 20])
        
        return limitations[:5]

    def _extract_future_work(self, structure: AcademicStructure) -> Optional[str]:
        """ä»Šå¾Œã®èª²é¡Œã®æŠ½å‡º"""
        if structure.conclusion:
            future_patterns = [
                r'(?:future work|future research|ä»Šå¾Œã®|å°†æ¥ã®)[\s\w]*(.{50,300}?)(?:\n\n|$)',
                r'(?:next step|æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—|ä»Šå¾Œ)[\s:]*(.{50,300}?)(?:\n\n|$)'
            ]
            
            for pattern in future_patterns:
                match = re.search(pattern, structure.conclusion, re.DOTALL | re.IGNORECASE)
                if match:
                    return match.group(1).strip()[:200]
        
        return None

    def _extract_mathematical_concepts(self, text: str) -> List[str]:
        """æ•°å­¦çš„æ¦‚å¿µã®æŠ½å‡º"""
        concepts = []
        
        for concept_type, patterns in self.technical_patterns.items():
            if concept_type == 'mathematical_concepts':
                for pattern in patterns:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    concepts.extend(matches)
        
        return list(set(concepts))[:10]

    def generate_academic_summary(self, file_path: Path, language: str = "ja", max_length: int = 200) -> Dict[str, Any]:
        """å­¦è¡“è«–æ–‡å°‚ç”¨è¦ç´„ç”Ÿæˆ"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = self.base_processor.process_file(file_path)
            if not extracted_text:
                raise ValueError("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # å­¦è¡“æ§‹é€ è§£æ
            structure = self.extract_academic_structure(extracted_text)
            
            # æŠ€è¡“å†…å®¹åˆ†æ
            technical_summary = self.analyze_technical_content(extracted_text, structure)
            
            # æ—¥æœ¬èªè¦ç´„ç”Ÿæˆ
            summary_parts = []
            
            # 1. åŸºæœ¬æƒ…å ±
            if structure.title:
                summary_parts.append(f"ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{structure.title}")
            
            if structure.authors:
                authors_str = "ã€".join(structure.authors[:5])
                summary_parts.append(f"ã€è‘—è€…ã€‘\n{authors_str}")
            
            # 2. æ–‡æ›¸åˆ†é¡
            doc_type_ja = {
                "research_paper": "ç ”ç©¶è«–æ–‡",
                "technical_report": "æŠ€è¡“å ±å‘Šæ›¸", 
                "patent": "ç‰¹è¨±æ–‡æ›¸",
                "manual": "æŠ€è¡“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«",
                "technical_document": "æŠ€è¡“æ–‡æ›¸"
            }
            
            level_ja = {
                "basic": "åŸºç¤ãƒ¬ãƒ™ãƒ«",
                "intermediate": "ä¸­ç´šãƒ¬ãƒ™ãƒ«", 
                "advanced": "ä¸Šç´šãƒ¬ãƒ™ãƒ«",
                "expert": "å°‚é–€å®¶ãƒ¬ãƒ™ãƒ«"
            }
            
            summary_parts.append(f"ã€æ–‡æ›¸ç¨®åˆ¥ã€‘\n{doc_type_ja.get(technical_summary.document_type, technical_summary.document_type)} ({level_ja.get(technical_summary.technical_level, technical_summary.technical_level)})")
            
            # 3. ä¸»è¦è²¢çŒ®
            summary_parts.append(f"ã€ä¸»è¦è²¢çŒ®ã€‘\n{technical_summary.main_contribution}")
            
            # 4. æ‰‹æ³•æ¦‚è¦
            if technical_summary.methodology_summary:
                summary_parts.append(f"ã€æ‰‹æ³•æ¦‚è¦ã€‘\n{technical_summary.methodology_summary}")
            
            # 5. ä¸»è¦ç™ºè¦‹
            if technical_summary.key_findings:
                findings_str = "\n".join([f"â€¢ {finding}" for finding in technical_summary.key_findings[:5]])
                summary_parts.append(f"ã€ä¸»è¦ç™ºè¦‹ã€‘\n{findings_str}")
            
            # 6. å®Ÿç”¨çš„å¿œç”¨
            if technical_summary.practical_applications:
                apps_str = "\n".join([f"â€¢ {app}" for app in technical_summary.practical_applications[:3]])
                summary_parts.append(f"ã€å®Ÿç”¨çš„å¿œç”¨ã€‘\n{apps_str}")
            
            # 7. æŠ€è¡“è©³ç´°
            if technical_summary.technical_details:
                details_str = "\n".join([f"â€¢ {detail}" for detail in technical_summary.technical_details[:3]])
                summary_parts.append(f"ã€æŠ€è¡“è©³ç´°ã€‘\n{details_str}")
            
            # 8. åˆ¶é™äº‹é …
            if technical_summary.limitations:
                limitations_str = "\n".join([f"â€¢ {limitation}" for limitation in technical_summary.limitations[:3]])
                summary_parts.append(f"ã€åˆ¶é™äº‹é …ã€‘\n{limitations_str}")
            
            # 9. ä»Šå¾Œã®èª²é¡Œ
            if technical_summary.future_work:
                summary_parts.append(f"ã€ä»Šå¾Œã®èª²é¡Œã€‘\n{technical_summary.future_work}")
            
            # 10. æ•°å­¦çš„æ¦‚å¿µ
            if technical_summary.mathematical_concepts:
                concepts_str = "ã€".join(technical_summary.mathematical_concepts[:8])
                summary_parts.append(f"ã€é–¢é€£æ¦‚å¿µã€‘\n{concepts_str}")
            
            # è¦ç´„çµ±åˆ
            full_summary = "\n\n".join(summary_parts)
            
            # é•·ã•èª¿æ•´
            if len(full_summary) > max_length * 20:  # æ¦‚ç®—æ–‡å­—æ•°åˆ¶é™
                # é‡è¦åº¦é †ã«å‰Šæ¸›
                essential_parts = summary_parts[:6]  # åŸºæœ¬æƒ…å ±ã€œå®Ÿç”¨çš„å¿œç”¨ã¾ã§
                full_summary = "\n\n".join(essential_parts)
            
            return {
                "summary": full_summary,
                "structure": structure,
                "technical_analysis": technical_summary,
                "processing_metadata": {
                    "document_type": technical_summary.document_type,
                    "technical_level": technical_summary.technical_level,
                    "sections_found": sum(1 for attr in ['title', 'authors', 'abstract', 'introduction', 'methodology', 'results', 'conclusion'] 
                                        if getattr(structure, attr) is not None),
                    "figures_count": len(structure.figures) if structure.figures else 0,
                    "tables_count": len(structure.tables) if structure.tables else 0,
                    "equations_count": len(structure.equations) if structure.equations else 0,
                    "references_count": len(structure.references) if structure.references else 0
                }
            }
            
        except Exception as e:
            logger.error(f"å­¦è¡“æ–‡æ›¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "summary": f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "structure": None,
                "technical_analysis": None,
                "processing_metadata": {"error": str(e)}
            }

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    processor = AcademicDocumentProcessor()
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    test_files = list(Path("data").glob("*.pdf"))[:1] if Path("data").exists() else []
    def process_technical_document_japanese(self, extracted_content: str, file_path: str = None) -> Dict[str, Any]:
        """
        æŠ€è¡“æ–‡æ›¸ã®è‹±æ—¥ç¿»è¨³ç‰¹åŒ–å‡¦ç†
        
        Args:
            extracted_content: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ç¿»è¨³å‡¦ç†çµæœè¾æ›¸
        """
        try:
            logger.info("ğŸ”„ æŠ€è¡“æ–‡æ›¸è‹±æ—¥ç¿»è¨³å‡¦ç†é–‹å§‹")
            
            # æŠ€è¡“ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹å‡¦ç†
            translation_result: TechnicalTranslationResult = self.technical_translator.translate_technical_document(
                extracted_content
            )
            
            # å­¦è¡“æ§‹é€ ã®æŠ½å‡ºï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã¨ã—ã¦ï¼‰
            academic_structure = self.extract_academic_structure(extracted_content)
            
            # æŠ€è¡“åˆ†æ
            technical_summary = self.analyze_technical_content(extracted_content, academic_structure)
            
            # çµæœã®çµ±åˆ
            result = {
                "japanese_translation": translation_result.japanese_translation,
                "document_type": translation_result.document_type,
                "technical_level": technical_summary.technical_level,
                "translation_quality": translation_result.translation_quality,
                "confidence_score": translation_result.confidence_score,
                "processing_time": translation_result.processing_time,
                "technical_terms_found": translation_result.technical_terms_found,
                "key_findings": technical_summary.key_findings,
                "practical_applications": technical_summary.practical_applications,
                "technical_details": technical_summary.technical_details,
                "main_contribution": technical_summary.main_contribution,
                "methodology_summary": technical_summary.methodology_summary,
                "mathematical_concepts": technical_summary.mathematical_concepts,
                "processing_metadata": {
                    "processor_type": "technical_translation",
                    "document_classification": translation_result.document_type,
                    "technical_level": technical_summary.technical_level,
                    "translation_confidence": translation_result.confidence_score,
                    "processing_time_seconds": translation_result.processing_time,
                    "terms_detected": len(translation_result.technical_terms_found),
                    "file_path": file_path or "unknown"
                }
            }
            
            logger.info(f"âœ… æŠ€è¡“ç¿»è¨³å®Œäº†: {translation_result.document_type} ({translation_result.translation_quality})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŠ€è¡“ç¿»è¨³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "japanese_translation": f"æŠ€è¡“ç¿»è¨³å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "document_type": "error",
                "technical_level": "unknown",
                "translation_quality": "error",
                "confidence_score": 0.0,
                "processing_time": 0.0,
                "technical_terms_found": [],
                "key_findings": [],
                "practical_applications": [],
                "technical_details": [],
                "main_contribution": "å‡¦ç†ã‚¨ãƒ©ãƒ¼",
                "methodology_summary": None,
                "mathematical_concepts": [],
                "processing_metadata": {
                    "processor_type": "technical_translation",
                    "error": str(e),
                    "file_path": file_path or "unknown"
                }
            }
    
    def create_enhanced_japanese_summary(self, extracted_content: str, file_path: str = None) -> str:
        """
        æ‹¡å¼µæ—¥æœ¬èªè¦ç´„ã®ç”Ÿæˆï¼ˆæŠ€è¡“ç¿»è¨³ç‰¹åŒ–ï¼‰
        
        Args:
            extracted_content: æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æ‹¡å¼µã•ã‚ŒãŸæ—¥æœ¬èªè¦ç´„æ–‡å­—åˆ—
        """
        try:
            # æŠ€è¡“ç¿»è¨³å‡¦ç†ã®å®Ÿè¡Œ
            translation_result = self.process_technical_document_japanese(extracted_content, file_path)
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿è¦ç´„ã®ç”Ÿæˆ
            enhanced_summary = self.formatter.format_technical_japanese_summary(translation_result)
            
            return enhanced_summary
            
        except Exception as e:
            logger.error(f"æ‹¡å¼µæ—¥æœ¬èªè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"æ‹¡å¼µè¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    import sys
    from pathlib import Path
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    processor = AcademicDocumentProcessor()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    test_files = list(Path(project_root / "data").glob("*.pdf"))
    
    if test_files:
        result = processor.generate_academic_summary(test_files[0])
        print("=" * 80)
        print("ğŸ“ å­¦è¡“è«–æ–‡è¦ç´„çµæœ")
        print("=" * 80)
        print(result["summary"])
        
        if result["processing_metadata"]:
            print("\n" + "=" * 40)
            print("ğŸ“Š å‡¦ç†ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
            print("=" * 40)
            for key, value in result["processing_metadata"].items():
                print(f"{key}: {value}")
        
        # æŠ€è¡“ç¿»è¨³ãƒ†ã‚¹ãƒˆ
        print("\n" + "=" * 80)
        print("ğŸ”¬ æŠ€è¡“ç¿»è¨³ç‰¹åŒ–å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        print("=" * 80)
        
        with open(test_files[0], 'rb') as f:
            # ã“ã“ã§PDFèª­ã¿è¾¼ã¿å‡¦ç†ãŒå¿…è¦
            content = "Sample technical content for translation testing."
            
        tech_result = processor.process_technical_document_japanese(content, str(test_files[0]))
        print(tech_result["japanese_translation"])
        
    else:
        print("ãƒ†ã‚¹ãƒˆç”¨PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
