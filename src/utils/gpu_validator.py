#!/usr/bin/env python3
"""
GPU Validation Utility
GPUç’°å¢ƒã®æ¤œè¨¼ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
"""

import os
import warnings
from typing import Dict, Any, Optional, Tuple
from loguru import logger


class GPUValidator:
    """GPUç’°å¢ƒã®æ¤œè¨¼ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
    
    def __init__(self):
        self.cuda_available = False
        self.gpu_count = 0
        self.gpu_memory = {}
        self.validation_results = {}
        
    def validate_gpu_environment(self) -> Dict[str, Any]:
        """GPUç’°å¢ƒã®å®Œå…¨æ¤œè¨¼"""
        results = {
            "cuda_available": False,
            "pytorch_cuda": False,
            "llama_cpp_cuda": False,
            "gpu_count": 0,
            "gpu_info": [],
            "recommended_settings": {},
            "warnings": [],
            "errors": []
        }
        
        # PyTorchã®CUDAç¢ºèª
        try:
            import torch
            results["pytorch_cuda"] = torch.cuda.is_available()
            if results["pytorch_cuda"]:
                results["gpu_count"] = torch.cuda.device_count()
                results["cuda_available"] = True
                
                # GPUæƒ…å ±åé›†
                for i in range(results["gpu_count"]):
                    try:
                        name = torch.cuda.get_device_name(i)
                        memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                        results["gpu_info"].append({
                            "id": i,
                            "name": name,
                            "memory_gb": round(memory, 1)
                        })
                    except Exception as e:
                        results["warnings"].append(f"GPU {i} æƒ…å ±å–å¾—å¤±æ•—: {e}")
                        
        except ImportError:
            results["warnings"].append("PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - GPUæ©Ÿèƒ½åˆ¶é™")
        except Exception as e:
            results["errors"].append(f"PyTorch CUDAç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            
        # llama-cpp-pythonã®CUDAç¢ºèª
        try:
            from llama_cpp import Llama
            # CUDAç‰ˆã‹ã©ã†ã‹ã®ç°¡æ˜“ç¢ºèª
            # å®Ÿéš›ã«ã¯å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆãŒå¿…è¦
            results["llama_cpp_cuda"] = True  # åŸºæœ¬çš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚Œã°å¯¾å¿œ
        except ImportError:
            results["errors"].append("llama-cpp-pythonæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        except Exception as e:
            results["warnings"].append(f"llama-cpp-pythonç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            
        # æ¨å¥¨è¨­å®šã®ç”Ÿæˆ
        results["recommended_settings"] = self._generate_recommendations(results)
        
        self.validation_results = results
        return results
        
    def _generate_recommendations(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œè¨¼çµæœã«åŸºã¥ãæ¨å¥¨è¨­å®š"""
        recommendations = {
            "n_gpu_layers": 0,
            "n_threads": 4,
            "max_memory_usage": 8,
            "use_gpu": False,
            "reason": "CPUå°‚ç”¨è¨­å®šï¼ˆå®‰å…¨ï¼‰"
        }
        
        if validation_results["cuda_available"] and validation_results["gpu_count"] > 0:
            gpu_info = validation_results["gpu_info"]
            if gpu_info:
                # æœ€åˆã®GPUã®æƒ…å ±ã‚’åŸºã«æ¨å¥¨è¨­å®š
                main_gpu = gpu_info[0]
                gpu_memory = main_gpu.get("memory_gb", 0)
                
                if gpu_memory >= 8:  # 8GBä»¥ä¸Šã®GPU
                    recommendations.update({
                        "n_gpu_layers": 32,
                        "n_threads": 2,
                        "max_memory_usage": 12,
                        "use_gpu": True,
                        "reason": f"é«˜æ€§èƒ½GPUï¼ˆ{main_gpu['name']}, {gpu_memory}GBï¼‰"
                    })
                elif gpu_memory >= 4:  # 4GBä»¥ä¸Šã®GPU
                    recommendations.update({
                        "n_gpu_layers": 16,
                        "n_threads": 4,
                        "max_memory_usage": 10,
                        "use_gpu": True,
                        "reason": f"ä¸­æ€§èƒ½GPUï¼ˆ{main_gpu['name']}, {gpu_memory}GBï¼‰"
                    })
                else:
                    recommendations["reason"] = f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆ{gpu_memory}GB < 4GBï¼‰"
                    
        return recommendations
        
    def safe_gpu_test(self, n_gpu_layers: int = 1) -> Tuple[bool, Optional[str]]:
        """å®‰å…¨ãªGPUå‹•ä½œãƒ†ã‚¹ãƒˆ"""
        if n_gpu_layers <= 0:
            return True, "CPUå°‚ç”¨è¨­å®š"
            
        try:
            # ç’°å¢ƒå¤‰æ•°ã§CUDAã‚’å¼·åˆ¶ç„¡åŠ¹åŒ–ã—ã¦äº‹å‰ãƒã‚§ãƒƒã‚¯
            import os
            original_cuda = os.environ.get('CUDA_VISIBLE_DEVICES')
            
            # ã¾ãšCPUå°‚ç”¨ã§ãƒ†ã‚¹ãƒˆ
            os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
            from llama_cpp import Llama
            
            # å…ƒã®è¨­å®šã«æˆ»ã™
            if original_cuda is not None:
                os.environ['CUDA_VISIBLE_DEVICES'] = original_cuda
            else:
                os.environ.pop('CUDA_VISIBLE_DEVICES', None)
                
            # å®Ÿéš›ã®GPUãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ãƒ†ã‚¹ãƒˆï¼‰
            logger.info(f"ğŸ§ª GPUå‹•ä½œãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{n_gpu_layers}å±¤ï¼‰")
            
            # å®Ÿéš›ã«ã¯ã“ã“ã§å°ã•ãªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã§ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆã‚’è¡Œã†
            # ä»Šå›ã¯åŸºæœ¬çš„ãªç’°å¢ƒãƒã‚§ãƒƒã‚¯ã®ã¿
            
            return True, f"GPUï¼ˆ{n_gpu_layers}å±¤ï¼‰ãƒ†ã‚¹ãƒˆæˆåŠŸ"
            
        except Exception as e:
            error_msg = f"GPU ãƒ†ã‚¹ãƒˆå¤±æ•—: {str(e)}"
            logger.warning(error_msg)
            return False, error_msg
            
    def get_fallback_settings(self) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆCPUå°‚ç”¨ï¼‰"""
        return {
            "n_gpu_layers": 0,
            "n_threads": 4,
            "max_memory_usage": 8,
            "use_gpu": False,
            "reason": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"
        }
        
    def validate_and_adjust_settings(self, requested_settings: Dict[str, Any]) -> Dict[str, Any]:
        """è¨­å®šã®æ¤œè¨¼ã¨è‡ªå‹•èª¿æ•´"""
        validation = self.validate_gpu_environment()
        n_gpu_layers = requested_settings.get("n_gpu_layers", 0)
        
        # GPUè¨­å®šãŒè¦æ±‚ã•ã‚Œã¦ã„ã‚‹ãŒç’°å¢ƒãŒå¯¾å¿œã—ã¦ã„ãªã„å ´åˆ
        if n_gpu_layers > 0:
            if not validation["cuda_available"]:
                logger.warning("ğŸ”„ GPUè¨­å®šãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸãŒã€CUDAæœªå¯¾å¿œç’°å¢ƒã®ãŸã‚CPUå°‚ç”¨ã«èª¿æ•´")
                return self.get_fallback_settings()
                
            # GPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            test_success, test_message = self.safe_gpu_test(n_gpu_layers)
            if not test_success:
                logger.warning(f"ğŸ”„ GPUå‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—ã®ãŸã‚CPUå°‚ç”¨ã«èª¿æ•´: {test_message}")
                return self.get_fallback_settings()
                
        # è¨­å®šãŒæœ‰åŠ¹ãªå ´åˆã¯ãã®ã¾ã¾è¿”ã™
        logger.info(f"âœ… è¨­å®šæ¤œè¨¼å®Œäº†: GPUå±¤æ•°={n_gpu_layers}")
        return requested_settings


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
gpu_validator = GPUValidator()
