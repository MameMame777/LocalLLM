#!/usr/bin/env python3
"""
Real PDF processing functions for LocalLLM GUI
Replaces mock functions with actual document processing and summarization
"""

import sys
import time
import re
import os
from pathlib import Path
from typing import Dict, Any, Optional
import traceback

# Fix encoding issues on Windows
if sys.platform == "win32":
    # Set console output encoding to UTF-8
    os.environ["PYTHONIOENCODING"] = "utf-8"
    
# Import loguru after encoding setup
from loguru import logger

# Safe logging functions to avoid Unicode issues
def safe_log_info(message: str):
    """Safe logging that avoids Unicode encoding issues"""
    try:
        # Remove Unicode characters that cause issues in Windows console
        clean_message = message.encode('ascii', 'ignore').decode('ascii')
        logger.info(clean_message)
    except Exception:
        # Fallback to print
        print(f"INFO: {message}")

def safe_log_warning(message: str):
    """Safe warning logging that avoids Unicode encoding issues"""
    try:
        clean_message = message.encode('ascii', 'ignore').decode('ascii')
        logger.warning(clean_message)
    except Exception:
        print(f"WARNING: {message}")

def safe_log_error(message: str):
    """Safe error logging that avoids Unicode encoding issues"""
    try:
        clean_message = message.encode('ascii', 'ignore').decode('ascii')
        logger.error(clean_message)
    except Exception:
        print(f"ERROR: {message}")

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# Import LocalLLM components
from src.document_processor import DocumentProcessor
from src.summarizer_enhanced import LLMSummarizer
from src.batch_processing.batch_processor import ProcessingResult
from src.utils.language_detector import LanguageDetector
from src.summarizer_enhanced import LLMSummarizer
from config.settings import get_settings
from src.batch_processing.batch_processor import ProcessingResult


def real_process_file_global(file_path: Path, **kwargs) -> ProcessingResult:
    """
    Global function for real file processing (pickle-compatible)
    
    Args:
        file_path: Path to the file to process
        **kwargs: Additional parameters including:
            - language: Target language for summary (default: 'ja')
            - max_length: Maximum summary length (default: 200)
            - output_dir: Output directory for individual files
            - use_llm: Whether to use actual LLM (default: True)
            
    Returns:
        ProcessingResult with actual processing outcome
    """
    start_time = time.time()
    
    try:
        # Extract parameters
        language = kwargs.get('language', 'ja')
        max_length = kwargs.get('max_length', 200)
        output_dir = kwargs.get('output_dir', None)
        use_llm = kwargs.get('use_llm', True)
        auto_detect_language = kwargs.get('auto_detect_language', False)
        
        # Initialize language detector
        if auto_detect_language or language == 'auto':
            lang_detector = LanguageDetector()
            safe_log_info(f"Auto-detecting language for: {file_path.name}")
        else:
            lang_detector = None
        
        # Validate file
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Get file info
        file_size = file_path.stat().st_size
        file_size_mb = file_size / (1024 * 1024)
        
        safe_log_info(f"Processing {file_path.name} ({file_size_mb:.2f} MB)")
        
        # Initialize processors
        doc_processor = DocumentProcessor()
        
        # Extract text content
        safe_log_info(f"Extracting text from {file_path}")
        extracted_text = doc_processor.process(str(file_path))
        
        if not extracted_text.strip():
            raise ValueError(f"No text content extracted from {file_path.name}")
        
        word_count = len(extracted_text.split())
        safe_log_info(f"Extracted {word_count} words from {file_path.name}")
        
        # Auto-detect language if enabled
        detected_source_lang = None
        final_target_lang = language
        
        if lang_detector and (auto_detect_language or language == 'auto'):
            try:
                detected_lang, confidence, detailed_info = lang_detector.detect_with_fallback(extracted_text)
                detected_source_lang = detected_lang
                
                safe_log_info(f"Language detected: {detected_lang} (confidence: {confidence:.2f})")
                
                # Determine final target language
                if language == 'auto':
                    final_target_lang = lang_detector.get_recommended_summary_language(detected_lang)
                    safe_log_info(f"Recommended summary language: {final_target_lang}")
                else:
                    final_target_lang = language
                    
            except Exception as e:
                safe_log_warning(f"Language detection failed: {e}, using default: {language}")
                final_target_lang = language if language != 'auto' else 'ja'
        else:
            final_target_lang = language if language != 'auto' else 'ja'
        
        # Generate summary if LLM is requested and available
        summary = ""
        if use_llm:
            try:
                settings = get_settings()
                # Try to find an available model
                model_path = _find_available_model()
                
                if model_path:
                    safe_log_info(f"Using LLM model: {model_path.name}")
                    summarizer = LLMSummarizer(str(model_path), settings)
                    
                    # Generate summary
                    summary = summarizer.summarize(extracted_text)
                    safe_log_info(f"Generated summary for {file_path.name}")
                else:
                    safe_log_warning("No LLM model found, generating extractive summary")
                    summary = _generate_extractive_summary(extracted_text, max_length, final_target_lang)
                    
            except Exception as e:
                safe_log_warning(f"LLM processing failed for {file_path.name}: {str(e)}")
                safe_log_info("Falling back to extractive summary")
                summary = _generate_extractive_summary(extracted_text, max_length, final_target_lang)
        else:
            # Generate simple extractive summary
            summary = _generate_extractive_summary(extracted_text, max_length, final_target_lang)
        
        # Save individual output file if output directory is specified
        output_file = None
        if output_dir:
            output_file = _save_individual_result(
                file_path, extracted_text, summary, output_dir, final_target_lang
            )
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        # Create detailed result
        result = ProcessingResult(
            file_path=file_path,
            status="success",
            summary=summary,
            processing_time=processing_time,
            metadata={
                'original_size_mb': file_size_mb,
                'word_count': word_count,
                'summary_length': len(summary.split()) if summary else 0,
                'requested_language': language,
                'final_target_language': final_target_lang,
                'detected_source_language': detected_source_lang,
                'language_auto_detected': bool(lang_detector and (auto_detect_language or language == 'auto')),
                'extraction_method': 'llm' if use_llm else 'extractive',
                'output_file': str(output_file) if output_file else None
            }
        )
        
        safe_log_info(f"Successfully processed {file_path.name} in {processing_time:.2f}s")
        return result
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = f"Failed to process {file_path.name}: {str(e)}"
        safe_log_error(f"{error_msg}")
        safe_log_error(f"Error details: {traceback.format_exc()}")
        
        return ProcessingResult(
            file_path=file_path,
            status="error",
            summary="",
            processing_time=processing_time,
            error=error_msg,
            metadata={
                'error_type': type(e).__name__,
                'original_size_mb': file_path.stat().st_size / (1024 * 1024) if file_path.exists() else 0
            }
        )


def _find_available_model() -> Optional[Path]:
    """Find the first available LLM model"""
    model_dir = Path("models")
    
    if not model_dir.exists():
        return None
    
    # Look for GGUF model files
    for model_file in model_dir.glob("*.gguf"):
        if model_file.exists() and model_file.stat().st_size > 1024 * 1024:  # At least 1MB
            return model_file
    
    return None


def _generate_extractive_summary(text: str, max_length: int = 200, target_language: str = 'ja') -> str:
    """
    Generate enhanced technical summary with detailed specifications
    
    Args:
        text: Input text
        max_length: Maximum number of words in summary
        target_language: Target language for summary ('ja', 'en', 'zh', 'ko')
        
    Returns:
        Enhanced technical summary with specifications
    """
    # Enhanced technical extraction patterns
    technical_patterns = {
        'specifications': [
            r'(?:ç²¾åº¦|accuracy)[:\s]*([0-9.]+%?[^.\n]*)',
            r'(?:é›»åœ§|voltage)[:\s]*([0-9.-]+\s*[VmM]?[Vv][^.\n]*)',
            r'(?:é›»æµ|current)[:\s]*([0-9.-]+\s*[Î¼umÎ¼mMA]?[Aa][^.\n]*)',
            r'(?:å‘¨æ³¢æ•°|frequency)[:\s]*([0-9.-]+\s*[kKmMgG]?[Hh][zZ][^.\n]*)',
            r'(?:æ¸©åº¦|temperature)[:\s]*([0-9.-]+\s*Â°?[Cc][^.\n]*)',
            r'(?:ãƒ¬ãƒ³ã‚¸|range)[:\s]*([0-9.-]+[^.\n]*)',
            r'(?:åˆ†è§£èƒ½|resolution)[:\s]*([0-9.-]+[^.\n]*)',
            r'(?:ãƒ“ãƒƒãƒˆ|bit)[:\s]*([0-9]+[^.\n]*)',
        ],
        'features': [
            r'(?:ç‰¹é•·|ç‰¹å¾´|features?)[:\s]*\n?([^ã€‚\n]+)',
            r'(?:æ©Ÿèƒ½|function)[:\s]*([^ã€‚\n]+)',
            r'(?:ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³|applications?)[:\s]*([^ã€‚\n]+)',
            r'(?:ç”¨é€”|use)[:\s]*([^ã€‚\n]+)',
        ],
        'technical_details': [
            r'(?:å…¥åŠ›|input)[:\s]*([^ã€‚\n]+)',
            r'(?:å‡ºåŠ›|output)[:\s]*([^ã€‚\n]+)',
            r'(?:å‹•ä½œ|operation)[:\s]*([^ã€‚\n]+)',
            r'(?:åˆ¶å¾¡|control)[:\s]*([^ã€‚\n]+)',
            r'(?:ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹|interface)[:\s]*([^ã€‚\n]+)',
        ]
    }
    
    # Extract technical information
    extracted_info = {
        'specifications': [],
        'features': [],
        'technical_details': []
    }
    
    for category, patterns in technical_patterns.items():
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                if len(match.strip()) > 5:  # Filter out too short matches
                    extracted_info[category].append(match.strip())
    
    # Build enhanced summary
    summary_parts = []
    
    # Add key specifications
    if extracted_info['specifications']:
        summary_parts.append("ã€ä¸»è¦ä»•æ§˜ã€‘")
        for spec in extracted_info['specifications'][:5]:  # Top 5 specifications
            summary_parts.append(f"â€¢ {spec}")
    
    # Add key features
    if extracted_info['features']:
        summary_parts.append("ã€ä¸»è¦æ©Ÿèƒ½ã€‘")
        for feature in extracted_info['features'][:3]:  # Top 3 features
            summary_parts.append(f"â€¢ {feature}")
    
    # Add technical details
    if extracted_info['technical_details']:
        summary_parts.append("ã€æŠ€è¡“è©³ç´°ã€‘")
        for detail in extracted_info['technical_details'][:3]:  # Top 3 details
            summary_parts.append(f"â€¢ {detail}")
    
    # If no technical info found, fall back to standard extractive summary
    if not any(extracted_info.values()):
        sentences = text.split('.')
        fallback_parts = []
        word_count = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            sentence_words = len(sentence.split())
            if word_count + sentence_words <= max_length:
                fallback_parts.append(sentence)
                word_count += sentence_words
            else:
                break
        
        extractive_summary = '. '.join(fallback_parts)
        if extractive_summary and not extractive_summary.endswith('.'):
            extractive_summary += '.'
    else:
        extractive_summary = '\n'.join(summary_parts)
    
    if not extractive_summary:
        extractive_summary = "No suitable content found for summary."
    
    # Apply translation if target language is different from detected source
    if target_language == 'ja' and not _is_japanese_text(extractive_summary):
        try:
            safe_log_info(f"Attempting translation to Japanese for text of length: {len(extractive_summary)}")
            translated_summary = _translate_to_japanese(extractive_summary)
            if translated_summary and len(translated_summary.strip()) > 10:
                safe_log_info(f"Translation successful. Result length: {len(translated_summary)}")
                return translated_summary
            else:
                safe_log_warning(f"Translation returned insufficient content: '{translated_summary[:50]}...'")
        except Exception as e:
            safe_log_warning(f"Translation failed: {e}, using original text")
    elif target_language == 'ja':
        safe_log_info(f"Text is already Japanese, no translation needed")
    else:
        safe_log_info(f"Target language is {target_language}, no translation needed")
    
    return extractive_summary


def _is_japanese_text(text: str) -> bool:
    """Check if text contains significant Japanese characters"""
    japanese_chars = sum(1 for char in text if '\u3040' <= char <= '\u309F' or 
                        '\u30A0' <= char <= '\u30FF' or 
                        '\u4E00' <= char <= '\u9FAF')
    total_chars = len([c for c in text if c.isalpha()])
    
    if total_chars == 0:
        return False
    
    return japanese_chars / total_chars > 0.1  # 10% threshold


def _translate_to_japanese(text: str) -> str:
    """
    Local translation to Japanese using pattern replacement and technical dictionaries
    
    Args:
        text: English text to translate
        
    Returns:
        Japanese translation (completely offline)
    """
    # Use OFFLINE translation only - no internet required!
    logger.info("ğŸ”’ Using offline translation (no internet connection required)")
    
    # Enhanced technical term translation dictionary for semiconductor/IC documents
    translation_patterns = {
        # Document types
        'Data Sheet': 'ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ',
        'data sheet': 'ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ',
        'datasheet': 'ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ',
        'Datasheet': 'ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ',
        
        # Basic electronics
        'Features': 'ç‰¹é•·',
        'features': 'ç‰¹é•·',
        'Specifications': 'ä»•æ§˜',
        'specifications': 'ä»•æ§˜',
        'Application': 'ç”¨é€”',
        'application': 'ç”¨é€”',
        'Applications': 'ç”¨é€”',
        'applications': 'ç”¨é€”',
        'Function': 'æ©Ÿèƒ½',
        'function': 'æ©Ÿèƒ½',
        'Performance': 'æ€§èƒ½',
        'performance': 'æ€§èƒ½',
        
        # AD637 specific (RMS converter)
        'RMS': 'RMS',
        'rms': 'RMS',
        'True RMS': 'çœŸã®RMS',
        'true rms': 'çœŸã®RMS',
        'RMS-to-DC': 'RMS-DCå¤‰æ›',
        'converter': 'ã‚³ãƒ³ãƒãƒ¼ã‚¿',
        'Converter': 'ã‚³ãƒ³ãƒãƒ¼ã‚¿',
        'logarithmic': 'å¯¾æ•°',
        'Logarithmic': 'å¯¾æ•°',
        'dB': 'dB',
        'decibel': 'ãƒ‡ã‚·ãƒ™ãƒ«',
        'Decibel': 'ãƒ‡ã‚·ãƒ™ãƒ«',
        'crest factor': 'ã‚¯ãƒ¬ã‚¹ãƒˆãƒ•ã‚¡ã‚¯ã‚¿',
        'Crest factor': 'ã‚¯ãƒ¬ã‚¹ãƒˆãƒ•ã‚¡ã‚¯ã‚¿',
        'effective value': 'å®ŸåŠ¹å€¤',
        'Effective value': 'å®ŸåŠ¹å€¤',
        'peak': 'ãƒ”ãƒ¼ã‚¯',
        'Peak': 'ãƒ”ãƒ¼ã‚¯',
        'average': 'å¹³å‡',
        'Average': 'å¹³å‡',
        
        # Voltage and current
        'voltage': 'é›»åœ§',
        'Voltage': 'é›»åœ§',
        'current': 'é›»æµ',
        'Current': 'é›»æµ',
        'supply': 'é›»æº',
        'Supply': 'é›»æº',
        'power': 'é›»åŠ›',
        'Power': 'é›»åŠ›',
        'consumption': 'æ¶ˆè²»',
        'Consumption': 'æ¶ˆè²»',
        
        # Precision and accuracy
        'accuracy': 'ç²¾åº¦',
        'Accuracy': 'ç²¾åº¦',
        'precision': 'ç²¾åº¦',
        'Precision': 'ç²¾åº¦',
        'resolution': 'åˆ†è§£èƒ½',
        'Resolution': 'åˆ†è§£èƒ½',
        'linearity': 'ç›´ç·šæ€§',
        'Linearity': 'ç›´ç·šæ€§',
        'nonlinearity': 'éç›´ç·šæ€§',
        'Nonlinearity': 'éç›´ç·šæ€§',
        'error': 'èª¤å·®',
        'Error': 'èª¤å·®',
        'drift': 'ãƒ‰ãƒªãƒ•ãƒˆ',
        'Drift': 'ãƒ‰ãƒªãƒ•ãƒˆ',
        'offset': 'ã‚ªãƒ•ã‚»ãƒƒãƒˆ',
        'Offset': 'ã‚ªãƒ•ã‚»ãƒƒãƒˆ',
        
        # Frequency and timing
        'frequency': 'å‘¨æ³¢æ•°',
        'Frequency': 'å‘¨æ³¢æ•°',
        'bandwidth': 'å¸¯åŸŸå¹…',
        'Bandwidth': 'å¸¯åŸŸå¹…',
        'response': 'å¿œç­”',
        'Response': 'å¿œç­”',
        'settling': 'ã‚»ãƒˆãƒªãƒ³ã‚°',
        'Settling': 'ã‚»ãƒˆãƒªãƒ³ã‚°',
        'time': 'æ™‚é–“',
        'Time': 'æ™‚é–“',
        
        # Signal types
        'input': 'å…¥åŠ›',
        'Input': 'å…¥åŠ›',
        'output': 'å‡ºåŠ›',
        'Output': 'å‡ºåŠ›',
        'signal': 'ä¿¡å·',
        'Signal': 'ä¿¡å·',
        'analog': 'ã‚¢ãƒŠãƒ­ã‚°',
        'Analog': 'ã‚¢ãƒŠãƒ­ã‚°',
        'digital': 'ãƒ‡ã‚¸ã‚¿ãƒ«',
        'Digital': 'ãƒ‡ã‚¸ã‚¿ãƒ«',
        'differential': 'å·®å‹•',
        'Differential': 'å·®å‹•',
        'single-ended': 'ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰',
        'Single-ended': 'ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰',
        'unipolar': 'å˜æ¥µæ€§',
        'Unipolar': 'å˜æ¥µæ€§',
        'bipolar': 'åŒæ¥µæ€§',
        'Bipolar': 'åŒæ¥µæ€§',
        
        # Temperature and environmental
        'temperature': 'æ¸©åº¦',
        'Temperature': 'æ¸©åº¦',
        'operating': 'å‹•ä½œ',
        'Operating': 'å‹•ä½œ',
        'ambient': 'å‘¨å›²',
        'Ambient': 'å‘¨å›²',
        'range': 'ãƒ¬ãƒ³ã‚¸',
        'Range': 'ãƒ¬ãƒ³ã‚¸',
        'condition': 'æ¡ä»¶',
        'Condition': 'æ¡ä»¶',
        'conditions': 'æ¡ä»¶',
        'Conditions': 'æ¡ä»¶',
        
        # Package and pins
        'package': 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸',
        'Package': 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸',
        'pin': 'ãƒ”ãƒ³',
        'Pin': 'ãƒ”ãƒ³',
        'pins': 'ãƒ”ãƒ³',
        'Pins': 'ãƒ”ãƒ³',
        'lead': 'ãƒªãƒ¼ãƒ‰',
        'Lead': 'ãƒªãƒ¼ãƒ‰',
        'SOIC': 'SOIC',
        'DIP': 'DIP',
        'PDIP': 'PDIP',
        
        # Common values
        'typical': 'æ¨™æº–',
        'Typical': 'æ¨™æº–',
        'minimum': 'æœ€å°',
        'Minimum': 'æœ€å°',
        'maximum': 'æœ€å¤§',
        'Maximum': 'æœ€å¤§',
        'nominal': 'å…¬ç§°',
        'Nominal': 'å…¬ç§°',
        'Information furnished': 'æä¾›ã•ã‚Œã‚‹æƒ…å ±',
        'information furnished': 'æä¾›ã•ã‚Œã‚‹æƒ…å ±',
        'believed to be accurate': 'æ­£ç¢ºã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¦ã„ã‚‹',
        'reliable': 'ä¿¡é ¼æ€§ã®ã‚ã‚‹',
        'responsibility': 'è²¬ä»»',
        'assumed': 'æƒ³å®šã•ã‚Œã‚‹',
        'infringements': 'ä¾µå®³',
        'patents': 'ç‰¹è¨±',
        'third parties': 'ç¬¬ä¸‰è€…',
        'subject to change': 'å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹',
        'without notice': 'äºˆå‘Šãªã—',
        'license': 'ãƒ©ã‚¤ã‚»ãƒ³ã‚¹',
        'granted': 'ä»˜ä¸ã•ã‚Œã‚‹',
        'implication': 'å«æ„',
        'otherwise': 'ãã®ä»–',
        'Trademarks': 'å•†æ¨™',
        'trademarks': 'å•†æ¨™',
        'registered': 'ç™»éŒ²',
        'property': 'è²¡ç”£',
        'respective': 'ãã‚Œãã‚Œã®',
        'owners': 'æ‰€æœ‰è€…',
        'Technology Way': 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ»ã‚¦ã‚§ã‚¤',
        'All rights reserved': 'å…¨æ¨©ç•™ä¿',
        'Rev.': 'æ”¹è¨‚ç‰ˆ',
        'PulSAR': 'PulSAR',
        '16-Bit': '16ãƒ“ãƒƒãƒˆ',
        '4-Channel': '4ãƒãƒ£ãƒ³ãƒãƒ«',
        '8-Channel': '8ãƒãƒ£ãƒ³ãƒãƒ«',
        '250 kSPS': '250 kSPS',
        'no missing codes': 'ãƒŸãƒƒã‚·ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ãªã—',
        'choice of inputs': 'å…¥åŠ›é¸æŠ',
        'Pseudo bipolar': 'ç–‘ä¼¼åŒæ¥µæ€§',
        'GND sense': 'GND ã‚»ãƒ³ã‚¹'
    }
    
    # Apply translations
    translated_text = text
    for english, japanese in translation_patterns.items():
        translated_text = translated_text.replace(english, japanese)
    
    # Basic sentence structure improvements
    sentence_patterns = {
        'However, no responsibility is assumed': 'ãŸã ã—ã€è²¬ä»»ã¯è² ã„ã¾ã›ã‚“',
        'for its use': 'ãã®ä½¿ç”¨ã«ã¤ã„ã¦',
        'that may result from its use': 'ãã®ä½¿ç”¨ã‹ã‚‰ç”Ÿã˜ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹',
        'No license is granted': 'ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯ä»˜ä¸ã•ã‚Œã¾ã›ã‚“',
        'by implication or otherwise': 'æš—ç¤ºã¾ãŸã¯ãã®ä»–ã®æ–¹æ³•ã«ã‚ˆã‚Š',
        'under any patent': 'ã„ã‹ãªã‚‹ç‰¹è¨±ã®ä¸‹ã§ã‚‚',
        'or patent rights': 'ã¾ãŸã¯ç‰¹è¨±æ¨©',
        'and registered trademarks': 'ãŠã‚ˆã³ç™»éŒ²å•†æ¨™',
        'are the property of': 'ã®è²¡ç”£ã§ã™',
        'their respective owners': 'ãã‚Œãã‚Œã®æ‰€æœ‰è€…',
        'with no missing codes': 'ãƒŸãƒƒã‚·ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ãªã—',
        'with choice of inputs': 'å…¥åŠ›é¸æŠæ©Ÿèƒ½ä»˜ã'
    }
    
    for english_phrase, japanese_phrase in sentence_patterns.items():
        translated_text = translated_text.replace(english_phrase, japanese_phrase)
    
    # Add contextual prefix for technical documents
    if any(term in text.lower() for term in ['adc', 'data sheet', 'analog devices', 'resolution', 'channel']):
        if not translated_text.startswith('ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ') and not translated_text.startswith('æŠ€è¡“æ–‡æ›¸'):
            translated_text = f"æŠ€è¡“æ–‡æ›¸è¦ç´„: {translated_text}"
    
    return translated_text
    translation_patterns = {
        # Common technical terms
        'Data Sheet': 'ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ',
        'Features': 'ç‰¹é•·',
        'Resolution': 'åˆ†è§£èƒ½',
        'Channel': 'ãƒãƒ£ãƒ³ãƒãƒ«',
        'Multiplexer': 'ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¯ã‚µ',
        'Unipolar': 'å˜æ¥µæ€§',
        'Differential': 'å·®å‹•',
        'Throughput': 'ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ',
        'Dynamic range': 'ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¬ãƒ³ã‚¸',
        'ADC': 'ADCï¼ˆã‚¢ãƒŠãƒ­ã‚°-ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰æ›å™¨ï¼‰',
        'ADCs': 'ADCï¼ˆã‚¢ãƒŠãƒ­ã‚°-ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰æ›å™¨ï¼‰',
        'Analog Devices': 'ã‚¢ãƒŠãƒ­ã‚°ãƒ»ãƒ‡ãƒã‚¤ã‚»ã‚º',
        'Technical Support': 'ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚µãƒãƒ¼ãƒˆ',
        'Document Feedback': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯',
        'Specifications': 'ä»•æ§˜',
        'LSB': 'LSBï¼ˆæœ€ä¸‹ä½ãƒ“ãƒƒãƒˆï¼‰',
        'INL': 'INLï¼ˆç©åˆ†éç›´ç·šæ€§ï¼‰',
        'SINAD': 'SINADï¼ˆä¿¡å·å¯¾é›‘éŸ³æ­ªã¿æ¯”ï¼‰',
        'bit resolution': 'ãƒ“ãƒƒãƒˆåˆ†è§£èƒ½',
        'missing codes': 'ãƒŸãƒƒã‚·ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰',
        'single-ended': 'ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ³ãƒ‰',
        'bipolar': 'åŒæ¥µæ€§',
        'maximum': 'æœ€å¤§',
        'typical': 'æ¨™æº–',
        'minimum': 'æœ€å°'
    }
    
    translated_text = text
    for english, japanese in translation_patterns.items():
        translated_text = translated_text.replace(english, japanese)
    
    # Add basic context if it looks like a technical document
    if any(term in text.lower() for term in ['adc', 'data sheet', 'analog devices', 'resolution']):
        translated_text = f"ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦: {translated_text}"
    
    return translated_text


def _save_individual_result(file_path: Path, extracted_text: str, summary: str, 
                          output_dir: str, language: str) -> Path:
    """
    Save individual processing result to file
    
    Args:
        file_path: Original file path
        extracted_text: Extracted text content
        summary: Generated summary
        output_dir: Output directory
        language: Target language
        
    Returns:
        Path to saved output file
    """
    output_path = Path(output_dir) / "processed"
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Generate output filename
    base_name = file_path.stem
    output_file = output_path / f"{base_name}_summary_{language}.md"
    
    # Create markdown content
    content = f"""# {file_path.name} - Processing Result

## File Information
- **Original File**: {file_path.name}
- **File Size**: {file_path.stat().st_size / (1024 * 1024):.2f} MB
- **Processing Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **Target Language**: {language}

## Summary
{summary}

## Extracted Text
```
{extracted_text[:2000]}{'...' if len(extracted_text) > 2000 else ''}
```

---
*Generated by LocalLLM Batch Processor*
"""
    
    # Write to file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return output_file


def create_real_processing_function(**default_params):
    """
    Create a real processing function with default parameters
    
    Args:
        **default_params: Default parameters for processing
        
    Returns:
        Configured processing function
    """
    def process_func(file_path: Path, **kwargs):
        # Merge default params with provided kwargs
        params = {**default_params, **kwargs}
        return real_process_file_global(file_path, **params)
    
    return process_func


# Test function for development
def test_real_processing():
    """Test the real processing function with a sample file"""
    import sys
    from pathlib import Path
    
    # Add src to path
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    
    # Test with data directory
    data_dir = Path("data")
    if data_dir.exists():
        # Find first PDF file
        for pdf_file in data_dir.glob("*.pdf"):
            print(f"ğŸ§ª Testing real processing with: {pdf_file.name}")
            
            result = real_process_file_global(
                file_path=pdf_file,
                language='ja',
                max_length=150,
                output_dir='output/test_real',
                use_llm=False  # Start with extractive summary
            )
            
            print(f"ğŸ“Š Result: {result.status}")
            print(f"ğŸ• Time: {result.processing_time:.2f}s")
            print(f"ğŸ“ Summary: {result.summary[:100]}...")
            break
    else:
        print("âŒ No data directory found for testing")


if __name__ == "__main__":
    test_real_processing()
